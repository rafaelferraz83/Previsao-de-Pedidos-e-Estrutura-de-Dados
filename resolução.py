# -*- coding: utf-8 -*-
"""Resolução.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KZAiw2LNyN_Ow8vtRj6lE5GKmoOxWqBl
"""

pip install catboost

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
import calendar

from pandas.tseries.offsets import MonthEnd
from google.colab import files
from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday, nearest_workday, MO
from pandas.tseries.offsets import CustomBusinessDay
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from scipy.stats import binom
from sklearn.preprocessing import OneHotEncoder, StandardScaler, QuantileTransformer, PowerTransformer
from scipy.stats import skew, kurtosis
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# === Regressors ===
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

"""# 1. Upload"""

uploaded = files.upload()

# Carregar os três arquivos
df_hist = pd.read_parquet('historical_orders.parquet')
df_aug_sales = pd.read_parquet('august_total_sales.parquet')
df_aug_missing = pd.read_parquet('august_with_missing_order_days.parquet')

# Visualizar as primeiras linhas de cada um (opcional)
print("historical_orders:")
display(df_hist.head(), df_hist.info())

print("\nVaugust_total_sales.parquet:")
display(df_aug_sales.head(), df_aug_sales.info())

print("\naugust_with_missing_order_days:")
display(df_aug_missing.head(), df_aug_missing.info())

df_hist_copy = df_hist.copy()
df_aug_sales_copy = df_aug_sales.copy()
df_aug_missing_copy =  df_aug_missing.copy()

"""# 2. Conversion of values and creation of attributes derived from the converted date
(Conversão de valores e criação de atributos oriundos da data convertida)

"""

# 1. Convert the date column to datetime format (Converte a coluna de data para o formato datetime)
df_hist_copy["order_date"] = pd.to_datetime(df_hist_copy["order_date"])

# 2. Create date-related features (Cria colunas relacionadas à data)
df_hist_copy["year"] = df_hist_copy["order_date"].dt.year  # Year (Ano)
df_hist_copy["month"] = df_hist_copy["order_date"].dt.month  # Month (Mês)
df_hist_copy["day"] = df_hist_copy["order_date"].dt.day  # Day of month (Dia do mês)
df_hist_copy["days_of_the_week"] = df_hist_copy["order_date"].dt.weekday  # Day of week (Dia da semana: 0=Mon, 6=Sun)
df_hist_copy["year_month"] = df_hist_copy["order_date"].dt.to_period("M")  # Year-Month (Ano-Mês)
df_hist_copy["order_day"] = df_hist_copy["order_date"].dt.date  # Only the date part (Apenas a parte da data)

# 3. Define Brazilian national holidays (Define feriados nacionais do Brasil)
br_holidays = [
    "2021-01-01",  # New Year's Day (Ano Novo)
    "2021-04-21",  # Tiradentes
    "2021-05-01",  # Labor Day (Dia do Trabalho)
    "2021-09-07",  # Independence Day (Independência)
    "2021-10-12",  # Our Lady of Aparecida (Padroeira do Brasil)
    "2021-11-02",  # All Souls' Day (Finados)
    "2021-11-15",  # Proclamation of the Republic (Proclamação da República)
    "2021-12-25",  # Christmas (Natal)
    # Add more years if needed (Adicione mais anos se necessário)
]

# 4. Convert holidays to datetime (Converte os feriados para datetime)
feriados_nacionais = pd.to_datetime(br_holidays)

# 5. Add weekend indicator (Adiciona indicador de fim de semana)
df_hist_copy["is_weekend"] = df_hist_copy["order_date"].dt.weekday >= 5  # Saturday or Sunday (Sábado ou Domingo)

# 6. Add holiday indicator (Adiciona indicador de feriado nacional)
df_hist_copy["is_holiday"] = df_hist_copy["order_date"].isin(feriados_nacionais)

# 7. Add business day indicator (Adiciona indicador de dia útil)
df_hist_copy["is_business_day"] = ~df_hist_copy["is_weekend"] & ~df_hist_copy["is_holiday"]  # Not weekend or holiday (Não é fim de semana nem feriado)

#df_hist_copy['year_month'] = df_hist_copy['year_month'].dt.to_timestamp()

df_hist_copy.info()

df_hist_copy.head(10)

df_hist_copy.describe(include=float)

# Data mínima
data_min = df_hist_copy['order_date'].min()

# Data máxima
data_max = df_hist_copy['order_date'].max()

# Data mais comum (moda)
data_moda = df_hist_copy['order_date'].mode().iloc[0] if not df_hist_copy['order_date'].mode().empty else None

print("Data mínima:", data_min.date())
print("Data máxima:", data_max.date())
print("Data mais comum (moda):", data_moda.date() if data_moda else None)

data_values = df_hist_copy['account_id'].nunique()
print(data_values)

"""#3. Orders with repeated value for the same customer on the same date.
(Pedidos com valor repetido para o mesmo cliente na mesma data.)
1. 1,427,999 duplicated transactions were identified.
(1.427.999 transações duplicadas foram identificadas.)
---
2. 38,082 unique customers were affected.(38.082 clientes únicos foram afetados.)
---
3. Most frequent duplicated values: 0.00 (30,180x) and -99.00 (13,981x). (Valores mais duplicados: 0.00 (30.180 vezes) e -99.00 (13.981 vezes).)
---
4. Even after removing 0.00 and -99.00, duplicates concentrate on Mon–Wed, especially in March–April. (Mesmo após remover 0.00 e -99.00, as duplicações se concentram de segunda a quarta, especialmente em março e abril.)
---
5. Over 1.1 million duplicates fall between R$0–200. (Mais de 1,1 milhão de duplicações estão entre R$0–200.)
---
6. Boxplots confirm low-value concentration (R$1–R$500) and consistent duplication across months. (Boxplots confirmam concentração em valores baixos (R$1–R$500).
---
7.
"""

# 1. Group by client, date and transaction value, count how many times each appears
# (Agrupar por cliente, data e valor da transação, contando quantas vezes cada combinação aparece)
grouped = df_hist_copy.groupby(
    ['account_id', 'order_date', 'transaction_amount']
).size().reset_index(name='count')

# 2. Keep only combinations that occur more than once (i.e., are duplicated)
# (Manter apenas combinações que ocorrem mais de uma vez – ou seja, são duplicadas)
duplicated_orders = grouped[grouped['count'] > 1]

# 3. Sum total number of duplicated rows across all cases
# (Somar o número total de linhas duplicadas em todos os casos)
total_duplicated_rows = duplicated_orders['count'].sum()

# 4. Count how many duplicated groups exist (client + date + value)
# (Contar quantos grupos duplicados existem – cliente + data + valor)
num_duplicated_groups = len(duplicated_orders)

# 5. Count how many distinct clients had duplications
# (Contar quantos clientes distintos tiveram duplicações)
num_clients_with_duplicates = duplicated_orders['account_id'].nunique()

# 6. Print summary
# (Imprimir resumo)
print(f"Total duplicated rows: {total_duplicated_rows}")
# (Total de linhas duplicadas: {total_duplicated_rows})

print(f"Number of duplicated groups (client + date + value): {num_duplicated_groups}")
# (Número de grupos duplicados (cliente + data + valor): {num_duplicated_groups})

print(f"Number of distinct clients with duplications: {num_clients_with_duplicates}")
# (Número de clientes distintos com duplicações: {num_clients_with_duplicates})

# 1. Get the minimum and maximum duplicated transaction values
# (Obter o menor e o maior valor de transação duplicado)
min_duplicated_value = duplicated_orders['transaction_amount'].min()
max_duplicated_value = duplicated_orders['transaction_amount'].max()

# 2. Count how many times each of them appears in duplicated records
# (Contar quantas vezes cada um aparece nos registros duplicados)
min_value_count = duplicated_orders[
    duplicated_orders['transaction_amount'] == min_duplicated_value
]['count'].sum()

max_value_count = duplicated_orders[
    duplicated_orders['transaction_amount'] == max_duplicated_value
]['count'].sum()

# 3. Print results
# (Imprimir os resultados)
print(f"Minimum duplicated value: {min_duplicated_value} – Occurrences: {min_value_count}")
# (Menor valor duplicado: {min_duplicated_value} – Ocorrências: {min_value_count})

print(f"Maximum duplicated value: {max_duplicated_value} – Occurrences: {max_value_count}")
# (Maior valor duplicado: {max_duplicated_value} – Ocorrências: {max_value_count})

# Top 20 most common duplicated transaction values
# (20 valores de transação mais comuns entre os duplicados)
top_20_values = duplicated_orders['transaction_amount'].value_counts().head(50)

print("Top 20 most frequent duplicated transaction values:")
print(top_20_values)

# 1.Convert 'order_date' to datetime if needed
# (Converter 'order_date' para datetime, se necessário)
duplicated_orders['order_date'] = pd.to_datetime(duplicated_orders['order_date'])

# 2.Create 'year_month' column with first day of the month
# (Criar a coluna 'year_month' com o primeiro dia do mês)
duplicated_orders['year_month'] = duplicated_orders['order_date'].dt.to_period('M').dt.to_timestamp()

# 3.Add a helper column to count each duplicated occurrence
# (Adicionar uma coluna auxiliar para contar cada ocorrência duplicada)
duplicated_orders['count'] = 1

# 4.Group by 'year_month' and sum the duplicate counts
# (Agrupar por 'year_month' e somar as contagens de duplicatas)
duplicates_per_month = duplicated_orders.groupby('year_month')['count'].sum().sort_index()

# 5.Create x-axis labels in mm/yyyy format
# (Criar os rótulos do eixo X no formato mm/aaaa)
labels = duplicates_per_month.index.strftime('%m/%Y')

# 6.Plot chart with formatted x-axis labels
# (Plotar o gráfico com os rótulos do eixo X formatados)
plt.figure(figsize=(12, 5))  # (Set figure size) (Definir tamanho da figura)
plt.bar(labels, duplicates_per_month.values)  # (Plot bar chart) (Plotar gráfico de barras)

plt.title("Duplicated Order Values per Month")         # (Set chart title) (Definir título do gráfico)
plt.xlabel("Month/Year")                               # (Set x-axis label) (Definir rótulo do eixo X)
plt.ylabel("Number of Duplicated Occurrences")         # (Set y-axis label) (Definir rótulo do eixo Y)
plt.xticks(rotation=45)                                # (Rotate x-axis labels) (Rotacionar rótulos do eixo X)
plt.grid(axis='y')                                     # (Add gridlines to y-axis) (Adicionar grade no eixo Y)
plt.tight_layout()                                     # (Adjust layout to avoid clipping) (Ajustar layout para evitar cortes)
plt.show()                                             # (Display the chart) (Exibir o gráfico)

# 1. Group by to find duplicated transaction values on same day per client
# (Criar DataFrame com duplicatas baseado no df_hist_copy)
duplicated_orders = df_hist_copy.groupby(
    ['account_id', 'order_date', 'transaction_amount']
).size().reset_index(name='count')

# 2. Filter only duplicated entries (count > 1)
# (Filtrar apenas duplicatas (count > 1))
duplicated_orders = duplicated_orders[duplicated_orders['count'] > 1]

# 3. Merge back with df_hist_copy to retrieve month and weekday info
# (Juntar de volta com df_hist_copy para herdar colunas como month e days_of_the_week)
duplicated_orders = duplicated_orders.merge(
    df_hist_copy[['account_id', 'order_date', 'transaction_amount', 'month', 'days_of_the_week']],
    on=['account_id', 'order_date', 'transaction_amount'],
    how='left'
)

# 4. Filter out 0.00 and -99.00 from duplicated orders
# (Remover os valores 0.00 e -99.00 das duplicatas)
filtered_duplicates = duplicated_orders[
    ~duplicated_orders['transaction_amount'].isin([0.00, -99.00])
]

# 5. Function to plot the heatmap
# (Função para gerar heatmap)
def plot_heatmap(df_sub, title, cmap):
    heatmap_data = df_sub.groupby(['month', 'days_of_the_week']).size().unstack().fillna(0)

    plt.figure(figsize=(12,6))
    sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap=cmap)
    plt.title(title)
    plt.xlabel('Weekday (0=Mon, 6=Sun)')  # (Dia da Semana (0=Seg, 6=Dom))
    plt.ylabel('Month')  # (Mês)
    plt.tight_layout()
    plt.show()

# 6. Plot heatmaps
# (Gerar heatmaps)
plot_heatmap(duplicated_orders, 'Heatmap – Duplicated Orders by Month and Day of the Week', cmap='Blues')
print()
plot_heatmap(filtered_duplicates, 'Heatmap – Filtered Duplicated Orders (Excluding 0.00 and -99.00)', cmap='Greens')

# Criar faixas de valor para transações duplicadas
duplicated_orders['value_range'] = pd.cut(
    duplicated_orders['transaction_amount'],
    bins=[-100, 0, 50, 200, 500, 1000, 5000, 10000, float('inf')],
    labels=['<=0', '0–50', '50–200', '200–500', '500–1k', '1k–5k', '5k–10k', '>10k']
)

# Contar ocorrências por faixa
duplicated_orders['value_range'].value_counts().sort_index()

# 1. Convert year_month to datetime format (Converte year_month para datetime)
df_hist_copy['year_month'] = df_hist_copy['year_month'] = df_hist_copy['year_month'].dt.to_timestamp()

# 2.Select float columns (Seleciona colunas numéricas do tipo float)
colunas_float = df_hist_copy.select_dtypes(include='float').columns.tolist()

# 3.Melt for global boxplot (Transforma o DataFrame em formato longo para gráfico geral)
df_long_global = df_hist_copy[colunas_float].melt(var_name='variable', value_name='value')

# 4.Melt with year_month for grouped boxplot (Transforma para gráfico agrupado por mês)
df_long_by_month = df_hist_copy[['year_month'] + colunas_float].melt(id_vars='year_month', var_name='variable', value_name='value')

# 5.Sort year_month (Ordena o campo year_month cronologicamente)
df_long_by_month = df_long_by_month.sort_values(by='year_month')
df_long_by_month['year_month_str'] = df_long_by_month['year_month'].dt.strftime('%Y-%m')  # Format for x-axis

# 6.Generate color palette and assign to variables (Gera paleta de cores e associa às variáveis)
palette_vars = sns.color_palette("Set2", n_colors=len(colunas_float))
palette_dict = dict(zip(colunas_float, palette_vars))

# 7.Create figure with two vertical plots (Cria figura com dois gráficos verticais)
fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(14, 12))

# 8.1.Plot 1: Global boxplot (Gráfico 1: boxplot geral)
sns.boxplot(
    data=df_long_global,
    x='variable',
    y='value',
    ax=ax1,
    palette=palette_dict
)
ax1.set_yscale('symlog')  # Symmetrical log scale (Escala logarítmica simétrica)
ax1.set_title('Boxplot of All Float Variables', fontsize=13)
ax1.set_xlabel('Variable')
ax1.set_ylabel('Value')
ax1.grid(True)
ax1.tick_params(axis='x', rotation=45)

# 8.2.Plot 2: Grouped by year_month and variable (Gráfico 2: agrupado por mês e variável)
sns.boxplot(
    data=df_long_by_month,
    x='year_month_str',
    y='value',
    hue='variable',
    ax=ax2,
    palette=palette_dict
)
ax2.set_yscale('symlog')
ax2.set_title('Boxplot Grouped by Month and Variable', fontsize=13)
ax2.set_xlabel('Year-Month')
ax2.set_ylabel('Value')
ax2.grid(True)
ax2.tick_params(axis='x', rotation=45)
ax2.legend(title='Variable', bbox_to_anchor=(1.01, 1), loc='upper left')

# 9.Final layout adjustments (Ajuste final de layout)
plt.tight_layout()
plt.show()

# 1. Identify duplicated values per client, day and amount
# (1. Identificar valores duplicados por cliente, dia e valor)
duplicated_orders = df_hist_copy.groupby(
    ['account_id', 'order_date', 'transaction_amount']
).size().reset_index(name='count')

# 2. Filter all groups that occur more than once (true duplicates)
# (2. Filtrar todos os grupos que ocorrem mais de uma vez — duplicatas reais)
duplicated_all = duplicated_orders[duplicated_orders['count'] > 1]

# 3. Get the unique keys to exclude from the original dataset
# (3. Obter as chaves únicas para excluir do conjunto original)
keys_to_exclude = duplicated_all[['account_id', 'order_date', 'transaction_amount']]

# 4. Remove all rows that match the duplicated keys using anti-join logic
# (4. Remover todas as linhas que correspondem às chaves duplicadas usando lógica de anti-join)
df_filtered = df_hist_copy.merge(
    keys_to_exclude,
    on=['account_id', 'order_date', 'transaction_amount'],
    how='left',
    indicator=True
)

df_filtered = df_filtered[df_filtered['_merge'] == 'left_only'].drop(columns=['_merge'])

# 5. Final result: filtered DataFrame with all duplicates removed
# (5. Resultado final: DataFrame filtrado com todas as duplicatas removidas)
print(f"Original shape: {df_hist_copy.shape}")
print(f"Filtered shape: {df_filtered.shape}")

df_hist_copy = df_filtered.copy()

"""# 4. Checking for negative, null, and zero values

## 4.1. Verification function
"""

# Function to diagnose missing, empty, zero and negative values in a DataFrame
def diagnostico_vazios(df):
    resultado = []  # (List to store results — Lista para armazenar os resultados)

    for col in df.columns:
        tipo = str(df[col].dtype)  # (Get column type as string — Obtém o tipo da coluna como string)
        total_linhas = len(df)  # (Total number of rows — Número total de linhas)

        # Initial counters
        nulos = df[col].isnull().sum()
        vazios = 0
        espacos = 0
        zeros = 0
        negativos = 0
        total_vazios = nulos  # will be adjusted (será ajustado)

        # For text columns (object or string)
        if tipo in ['object', 'string']:
            vazios = (df[col] == '').sum()  # (Empty strings — Strings vazias)
            espacos = (df[col].astype(str).str.strip() == '').sum()  # (Only spaces — Apenas espaços)
            total_vazios = nulos + espacos  # (Total nulls + spaces — Total de nulos + espaços)

        # For numeric columns
        elif 'int' in tipo or 'float' in tipo:
            zeros = (df[col] == 0).sum()  # (Zero values — Valores iguais a zero)
            negativos = (df[col] < 0).sum()  # (Negative values — Valores negativos)
            total_vazios = nulos + zeros  # (Total nulls + zeros — Total de nulos + zeros)

        # Append results (Adiciona os resultados ao dicionário)
        resultado.append({
             'column': col,  # (coluna)
             'type': tipo,  # (tipo)
             'nulls': nulos,  # (nulos)
             'empty_strings': vazios,  # (vazios)
             'only_spaces': espacos,  # (espaços)
             'zeros': zeros,  # (zeros)
             'negatives': negativos,  # (negativos)
             'total_empty': total_vazios,  # (total_vazios)
             'null_percentage': round(nulos / total_linhas * 100, 2),  # (percentual de nulos)
             'zero_percentage': round(zeros / total_linhas * 100, 2) if zeros > 0 else 0,  # (percentual de zeros)
             'negative_percentage': round(negativos / total_linhas * 100, 2) if negativos > 0 else 0  # (percentual de negativos)
        })

    # Create DataFrame and sort by total empty (Cria DataFrame e ordena pelo total de vazios)
    diagnostico_df = pd.DataFrame(resultado)
    diagnostico_df = diagnostico_df.sort_values(by='total_empty', ascending=False)

    return diagnostico_df  # (Return diagnostic table — Retorna a tabela de diagnóstico)

diagnostico = diagnostico_vazios(df_hist_copy)
display(diagnostico)

"""## 3.2. Analysis of zero or negative values

1. Users with zero-value orders:
(Usuários com pedidos de valor zero)
* A significant portion of users placed at least one order with a transaction amount equal to zero. This might reflect promotional offers, complimentary transactions, or system-related inconsistencies.
(Uma parcela significativa dos usuários realizou ao menos um pedido com valor igual a zero. Isso pode refletir ofertas promocionais, transações de cortesia ou inconsistências no sistema.)
---
2. Users with negative-value orders:
(Usuários com pedidos de valor negativo)
* Many users had transactions with negative amounts, which are commonly associated with refunds, cancellations, or system adjustments.
(Muitos usuários tiveram transações com valores negativos, geralmente associadas a reembolsos, cancelamentos ou erros sistêmicos.)
---
3. Users affected by zero or negative orders:
(Usuários afetados por pedidos zerados ou negativos)
* A large share of the user base experienced at least one transaction flagged as suspicious, which can influence financial analysis and predictive model training.
(Uma grande parcela dos usuários apresentou pelo menos uma transação marcada como suspeita, o que pode influenciar análises financeiras e o treinamento de modelos preditivos.)
---
4. Users with both zero and negative orders
(Usuários com pedidos tanto zerados quanto negativos)
* A smaller group of users had both zero and negative transactions, possibly suggesting anomalies, repeated operational issues, or potential misuse.
(Um grupo menor de usuários teve transações tanto zeradas quanto negativas, possivelmente sugerindo anomalias, problemas operacionais recorrentes ou uso indevido.)
"""

# 1. Total number of unique users
# (Total de usuários únicos)
total_id = df_hist_copy['account_id'].nunique()
print(f'Total number of unique users in the dataset: {total_id}')
print()

# 1. Users with at least one order with zero value
# (Usuários com pelo menos um pedido com valor zerado)
id_zero = df_hist_copy[df_hist_copy['transaction_amount'] == 0]['account_id'].unique()
print(f'Users with at least one **zero-value** order: {len(id_zero)}')
print(f'Proportion with **zero-value**: {len(id_zero) / total_id:.2%}')
print()

# 2. Users with at least one order with negative value
# (Usuários com pelo menos um pedido com valor negativo)
id_negative = df_hist_copy[df_hist_copy['transaction_amount'] < 0]['account_id'].unique()
print(f'Users with at least one **negative-value** order: {len(id_negative)}')
print(f'Proportion with **negative-value**: {len(id_negative) / total_id:.2%}')
print()

# 3. Users with at least one zero or negative order
# (Usuários com pelo menos um pedido zerado ou negativo)
id_zero_or_negative = df_hist_copy[df_hist_copy['transaction_amount'] <= 0]['account_id'].unique()
print(f'Users with at least one **zero or negative** order: {len(id_zero_or_negative)}')
print(f'Proportion affected (zero or negative): {len(id_zero_or_negative) / total_id:.2%}')
print()

# 4. Users with both zero and negative orders
# (Usuários com pedidos tanto zerados quanto negativos)
id_zero_and_negative = np.intersect1d(id_zero, id_negative)
print(f'Users with **both zero and negative** orders: {len(id_zero_and_negative)}')
print(f'Proportion with **both** (zero and negative): {len(id_zero_and_negative) / total_id:.2%}')

"""### 4.2.1 Negative value handling
1. These are transactions where the amount is negative, potentially indicating refunds, cancellations, or system errors.
(Essas são transações com valor negativo, o que pode indicar reembolsos, cancelamentos ou erros do sistema.)
---
2. This suggests that although there are many negative transactions, they are concentrated around a small number of repeated values.
(Isto sugere que, embora existam muitas transações negativas, elas estão concentradas em poucos valores repetidos.)
---
3. This concentration indicates that -99.00 is not a typical refund or business-related transaction value. Instead, it strongly resembles a placeholder or system error.
(Essa concentração indica que o valor -99.00 não é um reembolso típico ou transação de negócio. Em vez disso, se assemelha fortemente a um marcador de erro ou valor sintético.)
---
4. Heatmap 2 – Value -99.00 Only (Heatmap 2 – Apenas Valor -99.00)
* The -99.00 value appears massively concentrated in specific months: March, May, and October. (O valor -99.00 aparece massivamente concentrado em meses específicos: março, maio e outubro.)
* It peaks heavily on Wednesdays (weekday = 2), with counts reaching 15,000+ on some days. (Há picos intensos às quartas-feiras (weekday = 2), com contagens chegando a mais de 15.000 em alguns dias.)
* Other high frequencies occur on Tuesdays and Mondays, but to a lesser extent. (Outras frequências altas ocorrem em terças e segundas-feiras, mas em menor escala.)
* This behavior is systematic, showing non-random spikes that suggest automated processes or batch events.
(Este comportamento é sistemático, mostrando picos não aleatórios que sugerem processos automatizados ou eventos em lote.)
-- -
5. The data reveals that users, on average, increased their purchasing activity after receiving a -99 transaction.
(Os dados revelam que os usuários, em média, aumentaram sua atividade de compra após receberem uma transação de -99).
* Both the average and median number of unique order days per user were higher after the -99 event. (Tanto a média quanto a mediana de dias únicos com pedido por usuário foram maiores após o evento de -99).
* This suggests that the -99 transaction may be acting as a system intervention, such as a rollback, cashback or correction, which potentially re-engages users.(Isso sugere que a transação de -99 pode estar atuando como uma intervenção do sistema, como um rollback, cashback ou correção, que potencialmente reativa os usuários).
* Since the project aims to predict true user purchasing behavior, including this effect in the prediction could introduce bias related to internal system mechanics. (Como o projeto tem como objetivo prever o comportamento real de compra dos usuários, incluir esse efeito na previsão pode introduzir viés relacionado a mecanismos internos do sistema).
* Therefore, it is consistent to exclude -99 as a purchase, but still consider its monetary value in financial aggregations. (Portanto, é coerente excluir o -99 como pedido, mas ainda considerar seu valor monetário nas agregações financeiras).
---
6. All negative values, especially those equal to -99, will be kept in the dataset, as they may contain relevant transactional or operational context.
However, they will not be treated as valid orders when forecasting user demand or counting purchases. (Ao final, todos os valores negativos, principalmente os iguais a -99, serão mantidos no conjunto de dados, pois podem conter contexto transacional ou operacional relevante.
No entanto, não serão tratados como pedidos válidos nas previsões de demanda ou na contagem de compras.)
* These values will still be included in financial calculations, such as the sum of transaction amounts, to reflect possible monetary impacts.
(Esses valores ainda serão considerados nas análises financeiras, como a soma dos valores transacionados, para refletir impactos monetários potenciais.)
---
"""

df_negativos = df_hist_copy[df_hist_copy['transaction_amount'] < 0]
df_negativos.shape

# 1. Filter negative values (Filtra valores negativos)
valores_negativos = df_hist_copy[df_hist_copy['transaction_amount'] < 0]

# 2. Count unique negative values (Conta valores negativos únicos)
qtd_negativos_unicos = valores_negativos['transaction_amount'].nunique()

print(f"Total of unique negative values: {qtd_negativos_unicos}")  # (Total de valores negativos únicos)

# Count frequency of each negative value (Conta frequência de cada valor negativo)
top_20_negativos = valores_negativos['transaction_amount'].value_counts().head(20)

print("Top 20 most frequent negative values:")  # (Top 20 valores negativos mais frequentes:)
display(top_20_negativos)

# 1. Sort by the most negative values (Ordena pelos valores mais negativos)
maiores_negativos = valores_negativos.sort_values(by='transaction_amount')

# 2. Display top N most negative values (Exibe os N maiores valores negativos)
print("Most negative order values:")  # (Maiores valores negativos de pedidos:)
display(maiores_negativos.head(20))

# 1. Filter only rows with -99.00 values (Filtra apenas as linhas com valor -99.00)
valores_menos_99 = df_hist_copy[df_hist_copy['transaction_amount'] == -99.00]

# 2. Count occurrences by day (Conta ocorrências por dia)
frequencia_por_dia = valores_menos_99['order_date'].value_counts().sort_values(ascending=False)

# 3. Display the result (Exibe o resultado)
print("Top dates with most -99.00 values:")  # (Datas com mais ocorrências do valor -99.00:)
display(frequencia_por_dia.head(10))  # (Mostra as 10 principais datas)

# 1. Filter negative values (Filtra valores negativos)
valores_negativos = df_hist_copy[
    (df_hist_copy['transaction_amount'] < 0) &
    (df_hist_copy['transaction_amount'] != -99.00)
]
valores_menos_99 = df_hist_copy[df_hist_copy['transaction_amount'] == -99.00]

# 2. Fuction heatmap
def plot_heatmap(df_sub, title, cmap):
    heatmap_data = df_sub.groupby(['month', 'days_of_the_week']).size().unstack().fillna(0)
    plt.figure(figsize=(12,6))
    sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap=cmap)
    plt.title(title)
    plt.xlabel('Week (0=Mon, 6=Sun)')
    plt.ylabel('Month')
    plt.tight_layout()
    plt.show()

# 3. Negative value heatmap
plot_heatmap(valores_negativos, 'Heatmap – Negative Orders by Month and Day of the Week', cmap='Purples')
print()
# 3. 99 Negative value heatmap
plot_heatmap(valores_menos_99, 'Heatmap – 99 Negative Orders by Month and Day of the Week', cmap='Purples')

# 1. Filter only transactions with -99 (Filtra apenas transações com valor -99)
df_neg_99 = df_hist_copy[df_hist_copy['transaction_amount'] == -99]

# 2. Get first -99 date per user (Obtém a primeira data com -99 por usuário)
primeiro_neg_99 = df_neg_99.groupby('account_id')['order_date'].min().reset_index()
primeiro_neg_99.columns = ['account_id', 'primeira_data_neg_99']

# 3. Merge reference date into original data (Mescla a primeira data de -99 na base original)
df_merged = df_hist_copy.merge(primeiro_neg_99, on='account_id', how='inner')

# 4. Extract only the date component (Extrai apenas a data, sem horário)
df_merged['order_day'] = df_merged['order_date'].dt.date

# 5. Split before and after the -99 reference (Separa antes e depois da data do -99)
antes = df_merged[df_merged['order_date'] < df_merged['primeira_data_neg_99']]
depois = df_merged[df_merged['order_date'] > df_merged['primeira_data_neg_99']]

# 6. Count distinct order days per user (Conta dias únicos com pedido por usuário)
dias_antes = antes.groupby('account_id')['order_day'].nunique()
dias_depois = depois.groupby('account_id')['order_day'].nunique()

# 7. Compute statistics (Calcula estatísticas)
media_antes = dias_antes.mean()
mediana_antes = dias_antes.median()

media_depois = dias_depois.mean()
mediana_depois = dias_depois.median()

# 8. Print results (Exibe os resultados)
print(f"Average unique order days BEFORE first -99:  {media_antes:.2f}")     # (Média de dias com pedido ANTES do primeiro -99)
print(f"Median unique order days BEFORE first -99:   {mediana_antes:.2f}")   # (Mediana de dias com pedido ANTES do primeiro -99)

print(f"Average unique order days AFTER first -99:   {media_depois:.2f}")    # (Média de dias com pedido DEPOIS do primeiro -99)
print(f"Median unique order days AFTER first -99:    {mediana_depois:.2f}")  # (Mediana de dias com pedido DEPOIS do primeiro -99)

# 1. Filter transactions with value -99
# (Filtrar transações com valor -99)
df_neg_99 = df_hist_copy[df_hist_copy['transaction_amount'] == -99]

# 2. Count how many times -99 appears per client
# (Contar quantas vezes -99 aparece por cliente)
estornos_por_cliente = (
    df_neg_99.groupby('account_id')
    .size()
    .reset_index(name='qtd_estornos')
)

# 3. Get list of clients with -99
# (Obter a lista de clientes com -99)
clientes_com_estorno = estornos_por_cliente['account_id'].unique()

# 4. Filter positive transactions from those clients
# (Filtrar transações positivas desses clientes)
df_positivos = df_hist_copy[
    (df_hist_copy['account_id'].isin(clientes_com_estorno)) &
    (df_hist_copy['transaction_amount'] > 0)
]

# 5. Count distinct days with positive transactions
# (Contar dias distintos com pedidos válidos)
pedidos_positivos_distintos = (
    df_positivos.groupby('account_id')['order_date']
    .nunique()
    .reset_index()
    .rename(columns={'order_date': 'qtd_dias_com_pedidos'})
)

# 6. Sum of positive transaction values
# (Soma dos valores positivos)
soma_valores_positivos = (
    df_positivos.groupby('account_id')['transaction_amount']
    .sum()
    .reset_index(name='soma_valores_positivos')
)

# 7. Count number of distinct positive values
# (Contar número de valores positivos distintos)
qtd_valores_positivos_distintos = (
    df_positivos.groupby('account_id')['transaction_amount']
    .nunique()
    .reset_index(name='qtd_valores_positivos_distintos')
)

# 8. Merge all
# (Juntar todas as tabelas)
resultado = estornos_por_cliente.merge(pedidos_positivos_distintos, on='account_id', how='left')
resultado = resultado.merge(soma_valores_positivos, on='account_id', how='left')
resultado = resultado.merge(qtd_valores_positivos_distintos, on='account_id', how='left')

# 9. Show top 20 by -99 refunds
# (Mostrar os 20 clientes com mais estornos -99)
resultado_top_20 = resultado.sort_values('qtd_estornos', ascending=False).head(20)

# 10. Display
# (Exibir)
print("Top 20 clients with most -99 refunds and their valid order stats:")
# (Top 20 clientes com mais estornos -99 e estatísticas de seus pedidos válidos:)
display(resultado_top_20)

# 1. Filter only -99 transactions (Filtra apenas transações com valor -99)
df_neg_99 = df_hist_copy[df_hist_copy['transaction_amount'] == -99].copy()

# 2. Count number of -99 and unique days per client (Conta estornos e dias únicos por cliente)
qtd_por_cliente = df_neg_99['account_id'].value_counts().reset_index()
qtd_por_cliente.columns = ['account_id', 'qtd_99']

dias_por_cliente = df_neg_99.groupby('account_id')['order_date'].nunique().reset_index()
dias_por_cliente.columns = ['account_id', 'dias_unicos']

valor_negativo = df_neg_99.groupby('account_id')['transaction_amount'].sum().reset_index()
valor_negativo.columns = ['account_id', 'valor_negativo']

# 3. Merge info into a single DataFrame (Mescla todas as informações)
df_estornos = (
    qtd_por_cliente
    .merge(dias_por_cliente, on='account_id')
    .merge(valor_negativo, on='account_id')
)

# 4. Assign frequency band per client (Define faixa de estorno por cliente)
bins = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, float('inf')]
labels = ['1-5', '6-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80',
          '81-90', '91-100', '101-110', '111-120', '121-130', '131-140', '141-150', '>150']
df_estornos['faixa'] = pd.cut(df_estornos['qtd_99'], bins=bins, labels=labels, right=True)

# 5. Calculate total transaction value per client (Soma de todas as transações por cliente no frame completo)
valor_total = df_hist_copy.groupby('account_id')['transaction_amount'].sum().reset_index()
valor_total.columns = ['account_id', 'valor_total']

# 6. Merge total with estornos (Mescla com os dados de estorno)
df_estornos = df_estornos.merge(valor_total, on='account_id', how='left')

# 7. Group by faixa and summarize (Agrupa por faixa e resume)
resumo = df_estornos.groupby('faixa').agg(
    qtd_clientes=('account_id', 'count'),             # (quantidade de clientes)
    soma_total_valores=('valor_total', 'sum'),        # (valor total de transações)
    soma_total_negativos=('valor_negativo', 'sum')    # (valor total dos -99)
).reset_index()

# 8. Calculate difference (Calcula a diferença entre total e negativos)
resumo['diferenca'] = resumo['soma_total_valores'] - resumo['soma_total_negativos']

# 9. Exibe o resultado
display(resumo)

df_aug_missing_copy[df_aug_missing_copy['account_id'] == 	'BR_01658849590']

df_hist_copy.head()

"""### 4.2.2 Zero value handling
1.Heatmap – Zero-value Orders by Month and Day of the Week
(Heatmap – Pedidos com valor zero por Mês e Dia da Semana)
* The heatmap reveals a high and regular distribution of zero-value transactions across all months and weekdays, especially between January and July. (O heatmap revela uma distribuição alta e regular de transações com valor zero ao longo de todos os meses e dias da semana, especialmente entre janeiro e julho.)
* Unlike the pattern observed in the -99 values, the zero-value orders appear to be systematically spread, without abrupt spikes or isolated days. (Diferentemente do padrão observado nos valores -99, os pedidos com valor zero parecem estar espalhados de forma sistemática, sem picos abruptos ou dias isolados.)
* There is no evident anomaly or single-day concentration suggesting rollback or manual correction. (Não há anomalia evidente ou concentração em um único dia que sugira rollback ou correção manual.)
---
2. Zero-value orders per month
(Pedidos com valor zero por mês)
* The number of zero-value transactions shows a steady and increasing trend, peaking in December 2021 and again in July 2022. (O número de transações com valor zero mostra uma tendência crescente e consistente, com picos em dezembro de 2021 e novamente em julho de 2022.)
* There are no gaps or sudden drops, suggesting that these values are likely part of a recurrent process rather than anomalies or isolated failures. (Não há lacunas ou quedas bruscas, o que sugere que esses valores fazem parte de um processo recorrente, e não de falhas pontuais ou anomalias.)
* The volume remains high throughout the entire period, with all months consistently exceeding 7,000 zero-value orders. (O volume permanece alto durante todo o período, com todos os meses ultrapassando 7.000 pedidos com valor zero.)
---
3. Users with more zero-value orders
(Usuários com mais pedidos zerados):
* One user stands out with 440 zero orders, but only 148 positive orders, suggesting atypical behavior.
(Um usuário se destaca com 440 pedidos zerados, mas apenas 148 pedidos positivos, sugerindo um comportamento atípico.)
* Most other users have a higher or comparable number of positive orders, indicating that their zero-value transactions likely occur within normal usage patterns. (A maioria dos outros usuários apresenta número maior ou semelhante de pedidos positivos, indicando que as transações zeradas provavelmente ocorrem dentro de um padrão normal de uso.)
---
4. Analysis of zero-value order bands (Análise por faixas de pedidos com valor zero):
English:
* The consistent presence of zero-value orders indicates that they will be considered as cancellations rather than valid transactions. (A presença consistente de pedidos com valor zero indica que eles serão considerados como cancelamentos e não como transações válidas).

"""

# 1. Filter zero values (Filtra valores negativos)
df_zeros = df_hist_copy[df_hist_copy['transaction_amount'] == 0]

# 2. Fuction heatmap
def plot_heatmap(df_sub, title, cmap):
    heatmap_data = df_sub.groupby(['month', 'days_of_the_week']).size().unstack().fillna(0)
    plt.figure(figsize=(12,6))
    sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap=cmap)
    plt.title(title)
    plt.xlabel('Week (0=Mon, 6=Sun)')
    plt.ylabel('Month')
    plt.tight_layout()
    plt.show()

# 3. Negative value heatmap
plot_heatmap(df_zeros, 'Heatmap – Negative Orders by Month and Day of the Week', cmap='Purples')
print()

# Step 1: Count zero-value orders per month
# (Etapa 1: Conta os pedidos com valor zero por mês)
zero_orders_by_month = df_hist_copy[df_hist_copy['transaction_amount'] == 0]['year_month'].value_counts().sort_index()

# Step 2: Format the index for better x-axis labels
# (Etapa 2: Formata o índice para melhorar os rótulos do eixo x)
zero_orders_by_month.index = zero_orders_by_month.index.to_series().dt.strftime('%Y-%m')

# Step 3: Plot
# (Etapa 3: Plotagem)
zero_orders_by_month.plot(kind='bar', figsize=(12, 5))
plt.title('Zero-value orders per month')  # (Pedidos com valor zero por mês)
plt.xlabel('Year-Month')  # (Ano-Mês)
plt.ylabel('Number of zero orders')  # (Número de pedidos zerados)
plt.grid(True)
plt.tight_layout()
plt.show()

# 1. Get top 20 users with most zero-value orders (Obtém os 20 usuários com mais pedidos zerados)
top_usuarios_zeros = (
    df_hist_copy[df_hist_copy['transaction_amount'] == 0]
    .groupby('account_id')
    .size()
    .sort_values(ascending=False)
    .head(30)
)

# 2. Create a DataFrame (Cria um DataFrame)
top_usuarios_df = top_usuarios_zeros.reset_index()
top_usuarios_df.columns = ['account_id', 'qtd_pedidos_zero']

# 3. Calculate total positive spend per user (Calcula o total gasto com pedidos positivos por usuário)
gasto_positivo = (
    df_hist_copy[df_hist_copy['transaction_amount'] > 0]
    .groupby('account_id')['transaction_amount']
    .sum()
    .reset_index()
    .rename(columns={'transaction_amount': 'gasto_total'})
)

# 4. Count total positive orders (Conta o número total de pedidos positivos)
qtd_pedidos_positivos = (
    df_hist_copy[df_hist_copy['transaction_amount'] > 0]
    .groupby('account_id')
    .size()
    .reset_index(name='qtd_pedidos_positivo')
)

# 5. Count unique days with positive orders (Conta número de dias únicos com pedidos positivos)
dias_unicos_positivos = (
    df_hist_copy[df_hist_copy['transaction_amount'] > 0]
    .groupby('account_id')['order_date']
    .nunique()
    .reset_index()
    .rename(columns={'order_date': 'dias_unicos_pedido_positivo'})
)

# 6. Merge all information (Mescla todas as informações)
resultado = (
    top_usuarios_df
    .merge(qtd_pedidos_positivos, on='account_id', how='left')
    .merge(dias_unicos_positivos, on='account_id', how='left')
    .merge(gasto_positivo, on='account_id', how='left')
)

# 8. Display result (Exibe o resultado)
display(resultado)

# 1. Filter users with more zero-value orders than positive ones
# (Filtra usuários com mais pedidos zerados do que pedidos positivos)
usuarios_mais_zeros = resultado[resultado['qtd_pedidos_zero'] > resultado['qtd_pedidos_positivo']]

# 2. Display result (Exibe o resultado)
display(usuarios_mais_zeros)

df_hist[df_hist['account_id']=='BR_53953097000547']

df_aug_missing_copy[df_aug_missing_copy['account_id']=='BR_53953097000547']

df_aug_sales_copy[df_aug_sales_copy['account_id']=='BR_53953097000547']

# 1. Filter all zero-value orders (Filtra todos os pedidos zerados)
df_zeros = df_hist_copy[df_hist_copy['transaction_amount'] == 0].copy()

# 2. Count how many zero-value orders per client (Conta quantos pedidos zerados por cliente )
qtd_pedidos_zero = df_zeros.groupby('account_id').size().reset_index(name='qtd_pedidos_zero')

# 3. Count unique days with zero orders per client (Conta dias únicos com pedidos zerados por cliente)
dias_unicos_zero = (
    df_zeros.groupby('account_id')['order_date']
    .nunique()
    .reset_index()
    .rename(columns={'order_date': 'dias_unicos_zero'})
)

# 4. Total spend with positive orders (Gasto total com pedidos positivos)
gasto_positivo = (
    df_hist_copy[df_hist_copy['transaction_amount'] > 0]
    .groupby('account_id')['transaction_amount']
    .sum()
    .reset_index()
    .rename(columns={'transaction_amount': 'gasto_total'})
)

# 5. (Number of positive orders (Quantidade de pedidos positivos)
qtd_pedidos_positivos = (
    df_hist_copy[df_hist_copy['transaction_amount'] > 0]
    .groupby('account_id')
    .size()
    .reset_index(name='qtd_pedidos_positivo')
)

# 6. Unique days with positive orders (Dias únicos com pedidos positivos)
dias_unicos_positivos = (
    df_hist_copy[df_hist_copy['transaction_amount'] > 0]
    .groupby('account_id')['order_date']
    .nunique()
    .reset_index()
    .rename(columns={'order_date': 'dias_unicos_positivo'})
)

# 7. Merge all info (Junta todas as informações)
df_usuarios = (
    qtd_pedidos_zero
    .merge(dias_unicos_zero, on='account_id', how='left')
    .merge(qtd_pedidos_positivos, on='account_id', how='left')
    .merge(dias_unicos_positivos, on='account_id', how='left')
    .merge(gasto_positivo, on='account_id', how='left')
)

# 8. Define bands of zero-value orders (Define as faixas de pedidos zerados)
bins = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, float('inf')]
labels = ['1-5', '6-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90',
          '91-100', '101-110', '111-120', '121-130', '131-140', '141-150', '151-160', '161-170',
          '171-180', '181-190', '191-200', '>200']

df_usuarios['faixa'] = pd.cut(df_usuarios['qtd_pedidos_zero'], bins=bins, labels=labels, right=True)

# 9. Group by band and compute metrics (Agrupa por faixa e calcula métricas)
resumo = df_usuarios.groupby('faixa').agg(
    qtd_clientes=('account_id', 'count'),
    media_pedidos_positivo=('qtd_pedidos_positivo', 'mean'),
    media_dias_positivos=('dias_unicos_positivo', 'mean'),
    media_dias_zerados=('dias_unicos_zero', 'mean'),
    media_gasto=('gasto_total', 'mean')
).reset_index()

# 10. Round the values (Arredonda os valores)
resumo = resumo.round(2)

# 11. Display result (Exibe o resultado)
display(resumo)

df_hist_copy.head()

"""# 5. Dataframe"""

# 1. Aggregate transaction amount per client per day
# (1. Agregar o valor das transações por cliente e por dia)
df_agg = df_hist_copy.groupby([
    "account_id", "order_date", "year", "month", "day", "year_month",
    "days_of_the_week", "is_weekend", "is_holiday", "is_business_day"
]).agg({"transaction_amount": "sum"}).rename(
    columns={"transaction_amount": "total_transaction_amount"}  # (Renomear para total_transaction_amount)
).reset_index()

# 2. Remove 'order_day' column if present
# (2. Remover a coluna 'order_day' se existir)
if 'order_day' in df_agg.columns:
    df_agg = df_agg.drop(columns=['order_day'])

# 3. Create 'cutoff' column based on day of the month
# (3. Criar a coluna 'cutoff' com base no dia do mês)
def define_cutoff(day):
    if day <= 10:
        return 10
    elif day <= 20:
        return 20
    else:
        return 30

df_agg["cutoff"] = df_agg["day"].apply(define_cutoff)

# 4. Create 'has_order': 1 if total_transaction_amount > 0, else 0
# (4. Criar 'has_order': 1 se o valor total do dia for positivo, senão 0)
df_agg["has_order"] = df_agg["total_transaction_amount"].apply(
    lambda x: 1 if x > 0 else 0
)

# 5. Create column 'total_orders_in_month' based on has_order
# (5. Criar a coluna 'total_orders_in_month' com base em has_order)
orders_per_month = df_agg.groupby(["account_id", "year_month"])["has_order"] \
    .sum().reset_index().rename(columns={"has_order": "total_orders_in_month"})

# 6. Merge monthly totals back into df_agg
# (6. Juntar o total mensal de pedidos no DataFrame agregado)
df_agg = df_agg.merge(orders_per_month, on=["account_id", "year_month"], how="left")

# 7. Create 'qtd_orders': number of positive-value transactions per client per day
# (7. Criar 'qtd_orders': número de transações com valor positivo por cliente por dia)
df_pos = df_hist_copy[df_hist_copy["transaction_amount"] > 0]
qtd_orders = df_pos.groupby(["account_id", "order_date"]).size().reset_index(name="qtd_orders")

# 8. Merge qtd_orders into df_agg
# (8. Juntar qtd_orders no df_agg)
df_agg = df_agg.merge(qtd_orders, on=["account_id", "order_date"], how="left")
df_agg["qtd_orders"] = df_agg["qtd_orders"].fillna(0).astype(int)

# 9. Force qtd_orders = 0 where has_order == 0
# (9. Forçar qtd_orders = 0 onde has_order == 0)
df_agg.loc[df_agg["has_order"] == 0, "qtd_orders"] = 0

# 1. Get all columns
# (1. Obter todas as colunas)
cols = df_agg.columns.tolist()

# 2. Remove 'total_orders_in_month' and append it to the end
# (2. Remover 'total_orders_in_month' da lista e colocá-la no final)
cols.remove("total_orders_in_month")
cols.append("total_orders_in_month")

# 3. Reorder DataFrame columns
# (3. Reordenar as colunas do DataFrame)
df_agg = df_agg[cols]

df_agg.head()

df_agg.info()

# Check the distribution of the target column has_order
#(Verificar a distribuição da variável alvo has_order)
df_agg['has_order'].value_counts(normalize=True) * 100

# Validates that we're not counting orders on days where the client shouldn't have any.
#(Valida que não estamos contando pedidos em dias onde o cliente não deveria ter feito nenhum.)
df_agg[df_agg['has_order'] == 0]['qtd_orders'].unique()

df_agg[df_agg['has_order'] == 0].head()

df_agg[(df_agg['has_order'] == 0) & (df_agg['qtd_orders'] > 0)]

df_agg[
    (df_agg['account_id']=='BR_00002485176') &
    (df_agg['year_month']=='2022-06')
]

df_hist_copy[
    (df_hist_copy['account_id']=='BR_00002485176') &
    (df_hist_copy['year_month']=='2022-06')
]

df_agg[
    (df_agg['account_id']=='BR_00135741000180') &
    (df_agg['order_date']=='2022-06-01')
]

df_hist_copy[
    (df_hist_copy['account_id']=='BR_00135741000180') &
    (df_hist_copy['order_date']=='2022-06-01')
]

#Check for missing values (NaNs)
#(Verificar valores ausentes (NaNs))
df_agg.isna().sum()

df_agg[
    (df_agg['has_order'] == 1) &
    (df_agg['qtd_orders'] == 0)
]

# 1. Ensure 'order_date' is datetime
# (Garantir que 'order_date' esteja no formato datetime)
df_hist_agg['order_date'] = pd.to_datetime(df_hist_agg['order_date'], errors='coerce')

# 2. Create the real total of order days per user per month
# (Criar o total real de pedidos no mês para cada cliente (has_order == 1) )
total_orders = df_hist_agg[df_hist_agg["has_order"] == 1].groupby(
    ["account_id", "year_month"]
)["order_date"].nunique().reset_index().rename(columns={"order_date": "total_orders_in_month"})

# 3. Get unique combinations of user and year_month
# (Obter combinações únicas de usuário e ano-mês)
user_months = df_hist_agg[['account_id', 'year_month']].drop_duplicates()

# 4. Create all 3 cutoffs for each user and month
# (Criar todos os três cortes para cada combinação)
cutoffs = pd.DataFrame({'cutoff': [10, 20, 30]})
user_months = user_months.merge(cutoffs, how='cross')

# 5. Create base date for the month
# (Criar data base para o mês (dia 1º))
user_months['order_date'] = pd.to_datetime(user_months['year_month'].astype(str) + '-01')

# 6. Generate fake order_date based on cutoff
# (Gerar data fictícia baseada no corte)
def gerar_data_ficticia(row):
    if row['cutoff'] == 10:
        return row['order_date'] + pd.Timedelta(days=9)
    elif row['cutoff'] == 20:
        return row['order_date'] + pd.Timedelta(days=19)
    else:
        return row['order_date'] + MonthEnd(0)

user_months['order_date'] = user_months.apply(gerar_data_ficticia, axis=1)

# 7. Add default values
# Adicionar colunas padrão zeradas
user_months['total_transaction_amount'] = 0.0
user_months['qtd_orders'] = 0
user_months['has_order'] = 0
user_months['is_business_day'] = False
user_months['is_weekend'] = False
user_months['is_holiday'] = False
user_months['days_of_the_week'] = user_months['order_date'].dt.dayofweek
user_months['year'] = user_months['order_date'].dt.year
user_months['month'] = user_months['order_date'].dt.month
user_months['day'] = user_months['order_date'].dt.day

# 8. Merge the real order count per month
# (Juntar o total real de pedidos no mês)
user_months = user_months.merge(total_orders, on=["account_id", "year_month"], how="left")
user_months["total_orders_in_month"] = user_months["total_orders_in_month"].fillna(0).astype(int)

# 9. Merge with existing real data
# (Juntar com o df_hist_agg original (com registros reais)
df_completo = pd.merge(
    user_months,
    df_hist_agg,
    on=['account_id', 'year_month', 'cutoff'],
    how='left',
    suffixes=('_fake', '')
)

# 10. Fill missing values with defaults
# (Preencher os campos ausentes com os valores da base fake)
for col in ['order_date', 'total_transaction_amount', 'qtd_orders', 'has_order',
            'is_business_day', 'is_weekend', 'is_holiday',
            'days_of_the_week', 'year', 'month', 'day', 'total_orders_in_month']:
    df_completo[col] = df_completo[col].combine_first(df_completo[f'{col}_fake'])

# 11. Drop helper columns
# (Remover colunas auxiliares)
df_completo = df_completo.drop(columns=[col for col in df_completo.columns if col.endswith('_fake')])

df_completo.head(20)

# 1. Create a dictionary with real total orders per user and month
# (Criar um dicionário com o total real de pedidos por (account_id, year_month))
map_real_total_orders = total_orders.set_index(['account_id', 'year_month'])['total_orders_in_month'].to_dict()

# 2. Fix zero values in df_completo
# Corrigir valores zerados no df_completo
df_completo['total_orders_in_month'] = df_completo.apply(
    lambda row: map_real_total_orders.get((row['account_id'], row['year_month']), 0)
    if row['total_orders_in_month'] == 0 else row['total_orders_in_month'],
    axis=1
)

df_completo.head(20)

df_cuts_per_user_month = df_completo.groupby(['account_id', 'year_month'])['cutoff'].nunique()
print(df_cuts_per_user_month.value_counts().sort_index())

# Display all unique cutoff days (Mostra todos os dias de corte únicos)
print("Unique cutoff_day values (Valores únicos de cutoff_day):")
print(sorted(df_completo['cutoff'].unique()))

df_hist_agg = df_completo.copy()
df_hist_agg.head()

from google.colab import drive
drive.mount('/content/drive')

#Adjust this if needed
save_path = '/content/drive/MyDrive/Colab_Notebooks/df_hist_agg.csv'

# Save CSV
df_hist_agg.to_csv(save_path, index=False)

# Save Parquet
df_hist_agg.to_parquet(save_path.replace('.csv', '.parquet'), index=False)

# Print the total number of rows in the DataFrame (Imprime o número total de linhas no DataFrame)
print(f"Total number of rows: {df_hist_agg.shape[0]}")  # (Número total de linhas)

"""# 6. EAD
---
##DataFrame
* account_id: Unique identifier of the user account (Identificador único da conta do usuário).
* year_month: Month and year of the transaction period, formatted as a date (e.g., 2023-08-01) (Mês e ano do período da transação, formatado como data — ex.: 2023-08-01);
* cutoff_day: The simulation cutoff day used (10, 20 or last day of the month) (Dia de corte usado na simulação — 10, 20 ou último dia do mês);
* days_with_orders_so_far: Number of distinct days with at least one order until the cutoff day (Número de dias distintos com pelo menos um pedido até o dia de corte).
* total_orders_so_far: Total quantity of products purchased up to the cutoff day (Quantidade total de itens comprados até o dia de corte).
* total_amount_so_far: Total amount spent on purchases until the cutoff day (Valor total gasto em compras até o dia de corte).
* has_refund_in_cutoff: Boolean flag indicating if there was any refund before or on the cutoff day(Indicador booleano se houve algum reembolso antes ou no dia de corte).
*has_promo_in_cutoff: Boolean flag indicating if there was any promotion before or on the cutoff day (Indicador booleano se houve alguma promoção antes ou no dia de corte).
* total_days_in_month: Total number of calendar days in the current month
(Total de dias no mês atual).
* days_with_orders: Total number of distinct days in the month where purchases occurred (target) (Número total de dias distintos no mês com pelo menos um pedido)
---

1.  Graph observations (frequency):
* Most users have between 1 and 6 orders per month (A maioria dos usuários tem entre 1 e 6 pedidos por mês);
* Long tail behavior: some users exceed 10–15 orders (Comportamento de cauda longa: alguns usuários passam de 10–15 pedidos);
* The distribution appears discrete, skewed and decreasing (A distribuição parece discreta, assimétrica e decrescente.)
---
2. Corr:
* days_with_orders_so_far vs days_with_orders → 0.78: Strong positive correlation: The number of order days so far in the month is a good predictor of the total number of order days (Correlação forte e positiva: o número de dias com pedidos até o corte é um bom preditor do total de dias com pedidos no mês).
* total_orders_so_far vs days_with_orders_so_far → 0.63: Moderate to strong correlation: more order days tend to imply more orders in total (Correlação moderada a forte: quanto mais dias com pedidos, maior tende a ser a quantidade total de compras).
* total_orders_so_far vs days_with_orders → 0.51: Useful correlation: the total number of purchases so far gives some insight into the final target (Correlação relevante: o total de compras até o corte contribui para prever o número final de dias com pedidos).
* cutoff_day tem correlação baixa com days_with_orders → 0.00: As expected: the cutoff is arbitrary and shouldn't correlate with the final target (Como esperado: o corte é fixo e não está relacionado diretamente ao comportamento do cliente).
---
4. Scatter Plot:
* Most users placed orders on few days in the month (A maioria dos usuários fez pedidos em poucos dias do mês).
* There are cases where high volumes of products were purchased in just a few orders (Há casos em que volumes altos de produtos foram comprados em apenas alguns pedidos).
* There is a visible concentration of users with up to 10 order days, regardless of total volume purchased (Há uma concentração visível de usuários com até 10 dias de pedidos, independentemente do volume total comprado).
* The distribution across cutoff_day (color) appears well spread, suggesting the trend holds across all cutoffs. (A distribuição dos pontos segundo o cutoff_day está bem espalhada, sugerindo que a tendência se mantém para todos os cortes).
"""

from google.colab import drive
drive.mount('/content/drive')

!find /content/drive/MyDrive -name "df_hist_agg.csv"

# Path
df_simulated_pd = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/AB_InBev/df_hist_agg.csv')

df_hist_end = df_simulated_pd.copy()
df_hist_end.head()

df_hist_end[df_hist_end['total_orders_in_month'] ==0]

df_hist_end[(df_hist_end['total_orders_in_month']==0) & (df_hist_end['account_id']=='BR_00040707393')]

df_hist_end[(df_hist_end['total_orders_in_month']==0) & (df_hist_end['account_id']=='BR_00073009300')]

diagnostico = diagnostico_vazios(df_hist_end)
display(diagnostico)

df_hist_end.describe()

# Safe conversion to datetime without specifying format (Conversão segura para datetime sem especificar o formato)
df_hist_end.info()

# Convert numeric date-related features to integer
# (Converter variáveis numéricas de data para inteiro)
cols_to_int = ['year', 'month', 'day', 'days_of_the_week']
df_hist_end[cols_to_int] = df_hist_end[cols_to_int].astype(int)

df_hist_end['year_month'] = pd.to_datetime(df_hist_end['year_month'])

df_hist_end.info()

"""1. Sharp increase in transaction volume starting in 2022
(Crescimento acentuado do volume transacionado a partir de 2022)
* The total transaction value (blue line) shows a significant jump in March 2022, reaching the peak in the visualized period.
(O valor total transacionado (linha azul) mostra um salto expressivo em março de 2022, alcançando o pico no período visualizado.)
* This increase is noticeable even though the number of unique customers remains relatively stable, indicating a higher average ticket per customer.
(Esse aumento é visível mesmo com a quantidade de clientes únicos se mantendo relativamente estável, o que indica maior ticket médio por cliente.)

2. Stability in the number of unique customers after March 2022
(Estabilidade no número de clientes únicos após março de 2022)
* The orange bars, which represent the number of unique customers per month, gradually increase until March 2022 and then stabilize at a high level.
(As barras laranja, que representam a quantidade de clientes únicos por mês, aumentam gradualmente até março/2022 e se estabilizam em patamar elevado.)
* In other words, the customer base remains high but without continued growth after this point.
(Ou seja, o número de clientes se mantém alto, mas sem crescimento contínuo após esse ponto.)

3. Recovery trend starting in July 2021
(Tendência de recuperação a partir de julho de 2021)
* Between February 2021 and June 2021, there is stability or a slight drop in the transaction value.
(Entre fevereiro/2021 e junho/2021, há uma estabilidade ou leve queda no valor transacionado.)
* Recovery begins in July 2021 and accelerates until it peaks in March 2022.
(A recuperação começa em julho/2021 e acelera até atingir o pico em março/2022.)

4. Discrepancy between volume and customer count
(Descolamento entre volume e quantidade de clientes)
*The blue line (transaction value) varies more strongly than the bars (customers).
(A linha azul (valor transacionado) varia mais fortemente do que as barras (clientes).)
* This suggests that changes in the purchasing behavior of existing customers (more purchases or higher values) explain more than simple customer base growth.
(Isso sugere que mudanças no comportamento de compra dos clientes existentes (mais compras ou maiores valores) explicam mais do que simples crescimento da base.)

5. Conclusion for future modeling
(Conclusão para a modelagem futura)
* This chart shows that:
(Esse gráfico mostra que:)
   * Month and year can be strong predictors of volume.     (O mês e o ano podem ser fortes preditores do volume.)
   * Seasonal changes or external events (e.g., campaigns, pandemic, inflation) may have strongly influenced purchasing behavior.
     (Mudanças sazonais ou eventos externos (ex: campanhas, pandemia, inflação) podem ter influenciado fortemente o comportamento de compra.)
   * The number of unique customers alone does not explain the transaction value, so additional variables (such as average ticket or frequency) should be created and tested in modeling.
     (A quantidade de clientes únicos sozinha não explica o valor transacionado, portanto, variáveis adicionais (como ticket médio ou frequência) devem ser criadas e testadas na modelagem.)
"""

# 1. Aggregate total_transaction_amount per month
# (1. Agregar total_transaction_amount por mês)
amount_per_month = (
    df_hist_end.groupby('year_month')['total_transaction_amount']
    .sum()
    .reset_index()
)

# 2. Count unique account_id per month
# (2. Contar account_id únicos por mês)
unique_clients_per_month = (
    df_hist_end.groupby('year_month')['account_id']
    .nunique()
    .reset_index(name='unique_clients')
)

# 3. Merge both dataframes on year_month
# (3. Juntar os dois dataframes usando year_month)
df_plot = amount_per_month.merge(unique_clients_per_month, on='year_month')

# 4. Plot with two Y axes
# (4. Plotar com dois eixos Y)
fig, ax1 = plt.subplots(figsize=(12, 6))

# Primary Y axis: total_transaction_amount
# (Eixo Y primário: total_transaction_amount)
ax1.plot(df_plot['year_month'], df_plot['total_transaction_amount'], color='blue', marker='o', label='Total Transaction Amount')
ax1.set_ylabel('Total Transaction Amount', color='blue')  # (Valor total transacionado)
ax1.tick_params(axis='y', labelcolor='blue')

# Rotate x-axis labels
# (Rotacionar os rótulos do eixo X)
ax1.set_xticks(df_plot['year_month'])
ax1.set_xticklabels(df_plot['year_month'], rotation=45)

# Secondary Y axis: unique clients
# (Eixo Y secundário: clientes únicos)
ax2 = ax1.twinx()
ax2.bar(df_plot['year_month'], df_plot['unique_clients'], alpha=0.4, color='orange', label='Unique Clients')
ax2.set_ylabel('Number of Unique Clients', color='orange')  # (Número de clientes únicos)
ax2.tick_params(axis='y', labelcolor='orange')

# Add title and grid
# (Adicionar título e grade)
plt.title('Total Transaction Amount and Unique Clients per Month')  # (Total transacionado e clientes únicos por mês)
ax1.grid(axis='y')

# Adjust layout
# (Ajustar layout)
plt.tight_layout()
plt.show()

"""Distribuição relativamente balanceada entre os cortes
O número total de registros por corte é bem parecido (em torno de 550k–590k), o que garante que você terá uma base homogênea ao comparar modelos treinados em diferentes cortes (10, 20, 30).

Redução na proporção de has_order = 1 no corte 30

No corte 30, a taxa de has_order = 1 cai para 77%, comparado a 82% nos cortes 10 e 20.

Isso pode indicar que clientes que compram mais cedo no mês são mais representativos nos dados, e que à medida que o mês avança, há maior proporção de clientes inativos.

Tendência de crescimento da inatividade (has_order = 0)

A contagem de has_order = 0 aumenta do corte 10 para o 30.

Para fins de modelagem, especialmente se quiser prever se haverá novos pedidos no restante do mês, isso indica um aumento de incerteza com o tempo — e pode refletir em menor poder preditivo no corte 30.
"""

# 1. Count the total number of rows for each cutoff
# (1. Contar o número total de registros para cada corte)
cutoff_counts = df_hist_end['cutoff'].value_counts().sort_index()

# 2. Count how many rows have has_order == 1 per cutoff
# (2. Contar quantos registros têm has_order == 1 por corte)
cutoff_has_order_1 = df_hist_end[df_hist_end['has_order'] == 1]['cutoff'].value_counts().sort_index()

# 3. Count how many rows have has_order == 0 per cutoff
# (3. Contar quantos registros têm has_order == 0 por corte)
cutoff_has_order_0 = df_hist_end[df_hist_end['has_order'] == 0]['cutoff'].value_counts().sort_index()

# 4. Combine the counts into a single DataFrame
# (4. Combinar os resultados em um único DataFrame)
cutoff_summary = pd.DataFrame({
    'total_records': cutoff_counts,         # (total de registros)
    'has_order_1': cutoff_has_order_1,      # (quantos com has_order == 1)
    'has_order_0': cutoff_has_order_0       # (quantos com has_order == 0)
}).fillna(0).astype(int)  # Fill missing values with 0 and convert to int (Preencher valores ausentes com 0 e converter para inteiro)

# 5. Display the summary
# (5. Exibir o resumo)
print("Summary of records per cutoff (Resumo dos registros por corte):")
print(cutoff_summary)

# 1. Count the number of records for each value of has_order per cutoff
# (1. Contar o número de registros para cada valor de has_order por corte)
order_counts = df_hist_end.groupby(['cutoff', 'has_order']).size().unstack(fill_value=0)

# 2. Calculate total and percentages
# (2. Calcular o total e as porcentagens)
order_counts['total'] = order_counts[0] + order_counts[1]  # (Total de registros por corte)
order_counts['has_order_1'] = 100 * order_counts[1] / order_counts['total']  # (% com pedido)
order_counts['has_order_0'] = 100 * order_counts[0] / order_counts['total']  # (% sem pedido)

# 3. Keep only percentage columns
# (3. Manter apenas as colunas de porcentagem)
order_percent = order_counts[['has_order_1', 'has_order_0']].round(2)

# 4. Display the percentage table
# (4. Exibir a tabela de porcentagens)
print("Percentage of has_order = 1 and 0 per cutoff (Porcentagem de has_order por corte):")
print(order_percent)

"""1. Boxplot – Indicates presence of outliers
(Boxplot – Indica presença de outliers)
* Most values are concentrated between 1 and 6 orders per month.
(A maioria dos valores está concentrada entre 1 e 6 pedidos no mês.)
* The median (central line of the box) is close to 3 orders/month.
(A mediana (linha central da caixa) está próxima de 3 pedidos/mês.)
* There is a large number of outliers above 7 orders/month, reaching up to 26 orders.
(Há uma grande quantidade de outliers acima de 7 pedidos/mês, chegando até 26 pedidos.)
* These outliers may be affecting the model’s learning, especially linear models.
(Esses outliers podem estar afetando o aprendizado do modelo, principalmente modelos lineares.)
* Suggested action:
(Ação sugerida:)
   * Evaluate the impact of capping or transforming these values (e.g., log1p or PowerTransformer), or test models more robust to outliers.
(Avaliar o impacto de capar ou transformar esses valores (ex: log1p ou PowerTransformer), ou testar modelos mais robustos a outliers.)

2. Histogram – Skewed distribution (right-skewed)
(Histograma – Distribuição assimétrica (skewed à direita))
* The distribution is skewed to the right (right-skewed), i.e., there is a high concentration of low values (0 to 5), with a long tail to the right.
(A distribuição é assimétrica à direita (right-skewed), ou seja, há uma alta concentração de valores baixos (0 a 5), com cauda longa à direita.)
* The density curve confirms that values between 1 and 4 are the most frequent.
(A curva de densidade confirma que os valores entre 1 e 4 são os mais frequentes.)
* There are also many months with 0 orders (as seen in the zero bar).
(Há muitos meses com 0 pedidos também (como visto na barra do zero).)
* Suggested action:
   * The skewness suggests transforming the target variable (log1p, Box-Cox, Yeo-Johnson) to improve performance of linear models.
(A assimetria sugere o uso de transformações na variável alvo (log1p, Box-Cox, Yeo-Johnson) para melhorar o desempenho de modelos lineares.)
   * Separating the analysis by has_order may also help: those who had orders vs. those who didn’t.
(Separar a análise por has_order também pode ajudar: quem teve pedidos vs. quem não teve.)

3. Conclusion of the analysis
* The target variable does not follow a normal distribution, which impacts models sensitive to this (such as linear regression or SVR).
(A variável alvo não segue uma distribuição normal, o que afeta modelos sensíveis a isso (como regressão linear ou SVR).)
* There is a significant presence of outliers.
(Há presença significativa de outliers.)
"""

# 1. Keep only one row per client and month (to avoid duplicates)
# (1. Manter apenas uma linha por cliente e mês - para evitar duplicatas)
df_target = df_hist_end.drop_duplicates(subset=['account_id', 'year_month'])[['account_id', 'year_month', 'total_orders_in_month']]

# 2. Create the plots
# (2. Criar os gráficos)
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 3. Boxplot
# (3. Boxplot)
sns.boxplot(y='total_orders_in_month', data=df_target, ax=axes[0])
axes[0].set_title('Boxplot of Total Orders in Month')  # (Boxplot do Total de Pedidos no Mês)
axes[0].set_ylabel('Total Orders')  # (Total de Pedidos)

# 4. Histogram
# (4. Histograma)
sns.histplot(df_target['total_orders_in_month'], bins=20, kde=True, ax=axes[1])
axes[1].set_title('Distribution of Total Orders in Month')  # (Distribuição do Total de Pedidos no Mês)
axes[1].set_xlabel('Total Orders')  # (Total de Pedidos)

# 5. Show plots
# (5. Exibir os gráficos)
plt.tight_layout()
plt.show()

# Select numeric columns only (Seleciona apenas colunas numéricas)
numeric_cols = df_hist_end.select_dtypes(include=['number'])

# Compute correlation matrix (Calcula a matriz de correlação)
corr_matrix = numeric_cols.corr()

# Set up the matplotlib figure (Configura a figura do matplotlib)
plt.figure(figsize=(10, 8))

# Draw the heatmap (Desenha o mapa de calor)
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=0.5)

plt.title("Correlation Matrix of Numeric Features (Matriz de Correlação das Variáveis Numéricas)")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""# 7. Feature Engineering"""

# Separate data by cutoff.
# (Separar os dados por corte)
df_corte_10 = df_hist_end[df_hist_end['cutoff'] == 10].copy()
df_corte_20 = df_hist_end[df_hist_end['cutoff'] == 20].copy()
df_corte_30 = df_hist_end[df_hist_end['cutoff'] == 30].copy()

df_corte_10.head()

# Number of days with order
# (Número de dias com pedido (soma do has_order))
df_corte_10['num_days_with_order'] = (
    df_corte_10.groupby(['account_id', 'year_month'])['has_order']
    .transform('sum')
)

df_corte_20['num_days_with_order'] = (
    df_corte_20.groupby(['account_id', 'year_month'])['has_order']
    .transform('sum')
)

df_corte_30['num_days_with_order'] = (
    df_corte_30.groupby(['account_id', 'year_month'])['has_order']
    .transform('sum')
)

# Create column: cutoff_day if order, else 0
# (Criar coluna 'cutoff_day_if_order')
df_corte_10['cutoff_day_if_order'] = np.where(
    df_corte_10['has_order'] == 1,
    df_corte_10['cutoff'] -  df_corte_10['day'],
    0
)

df_corte_20['cutoff_day_if_order'] = np.where(
    df_corte_20['has_order'] == 1,
    df_corte_20['cutoff'] -  df_corte_20['day'],
    0
)

df_corte_30['cutoff_day_if_order'] = np.where(
    df_corte_30['has_order'] == 1,
    df_corte_30['cutoff'] -  df_corte_30['day'],
    0
)

# Ticket médio por pedido
# (Average ticket per order — avoid division by zero)
df_corte_10['avg_ticket_per_day'] = np.where(
    df_corte_10['qtd_orders'] == 0, 0,
    df_corte_10['total_transaction_amount'] / df_corte_10['qtd_orders'],
).round(2)

df_corte_20['avg_ticket_per_day'] = np.where(
    df_corte_20['qtd_orders'] == 0, 0,
    df_corte_20['total_transaction_amount'] / df_corte_20['qtd_orders'],
).round(2)

df_corte_30['avg_ticket_per_day'] = np.where(
    df_corte_30['qtd_orders'] == 0, 0,
    df_corte_30['total_transaction_amount'] / df_corte_30['qtd_orders'],

).round(2)

# Calculates the average amount spent per cut
# (Calcula a medía de valores gastos por corte)
df_corte_10['avg_ticket_per_cutoff'] = (df_corte_10['total_transaction_amount'] / 10).round(2)

df_corte_20['avg_ticket_per_cutoff'] = (df_corte_20['total_transaction_amount'] / 10).round(2)

df_corte_30['avg_ticket_per_cutoff'] = (df_corte_30['total_transaction_amount'] / 10).round(2)

# Calculate the monthly conversion rate as the ratio between days with order and total days until cutoff
# (Calcula a taxa de conversão mensal como a razão entre os dias com pedido e o total de dias até o corte)
df_corte_10['conversion_rate_10_days'] = (df_corte_10['num_days_with_order'] / 10).round(2)

df_corte_20['conversion_rate_10_days'] = (df_corte_20['num_days_with_order'] / 10).round(2)

df_corte_30['conversion_rate_10_days'] = (df_corte_30['num_days_with_order'] / 10).round(2)

# Calculate recency_score based on whether a purchase was made
# (Calcula a recency_score com base se houve pedido ou não)
df_corte_10['recency_score'] = np.where(
    df_corte_10['has_order'] == 1,
    1 - (df_corte_10['cutoff_day_if_order'] / 10),
    0
)

df_corte_20['recency_score'] = np.where(
    df_corte_20['has_order'] == 1,
    1 - (df_corte_20['cutoff_day_if_order'] / 10),
    0
)

df_corte_30['recency_score'] = np.where(
    df_corte_30['has_order'] == 1,
    1 - (df_corte_30['cutoff_day_if_order'] / 10),
    0
)

# Estimated probability of future orders based on current frequency
# (Probabilidade estimada de novos pedidos com base na frequência atual)
# Probability that the customer will make new orders in the remaining days, assuming the current rate continues
# (Probabilidade de o cliente realizar novos pedidos nos dias restantes, assumindo que a taxa atual se mantém)
df_corte_10['remaining_days'] = 30 - df_corte_10['cutoff']  # Number of days left in the month (Número de dias restantes no mês)
df_corte_10['p_future_order'] = (df_corte_10['num_days_with_order'] * df_corte_10['remaining_days']) / (df_corte_10['cutoff'] ** 2)

df_corte_20['remaining_days'] = 30 - df_corte_20['cutoff']  # Number of days left in the month (Número de dias restantes no mês)
df_corte_20['p_future_order'] = (df_corte_20['num_days_with_order'] * df_corte_20['remaining_days']) / (df_corte_20['cutoff'] ** 2)

df_corte_30['remaining_days'] = 30 - df_corte_30['cutoff']  # Number of days left in the month (Número de dias restantes no mês)
df_corte_30['p_future_order'] = (df_corte_30['num_days_with_order'] * df_corte_30['remaining_days']) / (df_corte_30['cutoff'] ** 2)

# Average number of orders per day with order (intensity of purchase)
# (Média de pedidos por dia com pedido - intensidade da compra)
# How many orders the customer makes on average in the days when they place at least one order
# (Quantos pedidos o cliente faz em média nos dias em que realiza pelo menos um pedido)
df_corte_10['avg_orders_per_order_day'] = np.where(
    df_corte_10['num_days_with_order'] == 0, 0,
    df_corte_10['qtd_orders'] / df_corte_10['num_days_with_order']
)

df_corte_20['avg_orders_per_order_day'] = np.where(
    df_corte_20['num_days_with_order'] == 0, 0,
    df_corte_20['qtd_orders'] / df_corte_20['num_days_with_order']
)

df_corte_30['avg_orders_per_order_day'] = np.where(
    df_corte_30['num_days_with_order'] == 0, 0,
    df_corte_30['qtd_orders'] / df_corte_30['num_days_with_order']
)

# Probability of having at least 5 orders in the month (binomial model)
# (Probabilidade de ter pelo menos 5 pedidos no mês - modelo binomial)
# Estimated daily success probability for orders based on current data
# (Probabilidade estimada de sucesso diário com base no comportamento atual)
# Probability of reaching 5 or more order days in the full month, assuming binomial distribution
# (Probabilidade de atingir 5 ou mais dias com pedido no mês inteiro, assumindo distribuição binomial)
df_corte_10['p_binom'] = df_corte_10['num_days_with_order'] / df_corte_10['cutoff']
df_corte_10['p_at_least_5_orders_binom'] = binom.sf(k=4, n=30, p=df_corte_10['p_binom'])

df_corte_20['p_binom'] = df_corte_20['num_days_with_order'] / df_corte_20['cutoff']
df_corte_20['p_at_least_5_orders_binom'] = binom.sf(k=4, n=30, p=df_corte_20['p_binom'])

df_corte_30['p_binom'] = df_corte_30['num_days_with_order'] / df_corte_30['cutoff']
df_corte_30['p_at_least_5_orders_binom'] = binom.sf(k=4, n=30, p=df_corte_30['p_binom'])

#interaction of attributes
df_corte_10['num_days_with_order_X_cutoff_day_if_order'] = df_corte_10['num_days_with_order'] * df_corte_10['cutoff_day_if_order']

df_corte_20['num_days_with_order_X_cutoff_day_if_order'] = df_corte_20['num_days_with_order'] * df_corte_20['cutoff_day_if_order']

df_corte_30['num_days_with_order_X_cutoff_day_if_order'] = df_corte_30['num_days_with_order'] * df_corte_30['cutoff_day_if_order']

###############################################################################################################################################

df_corte_10['total_transaction_amount_X_cutoff_day_if_order'] = np.where(
    df_corte_10['cutoff_day_if_order'] == 0, 0,
    df_corte_10['total_transaction_amount'] * df_corte_10['cutoff_day_if_order']
).round(2)

df_corte_20['total_transaction_amount_X_cutoff_day_if_order'] = np.where(
    df_corte_20['cutoff_day_if_order'] == 0, 0,
    df_corte_20['total_transaction_amount'] * df_corte_20['cutoff_day_if_order']
).round(2)

df_corte_30['total_transaction_amount_X_cutoff_day_if_order'] = np.where(
    df_corte_30['cutoff_day_if_order'] == 0, 0,
    df_corte_30['total_transaction_amount'] * df_corte_30['cutoff_day_if_order']
).round(2)

###############################################################################################################################################

df_corte_10['day_X_has_order'] = np.where(
    df_corte_10['num_days_with_order'] == 0, 0,
    df_corte_10['num_days_with_order'] / df_corte_10['day']
)

df_corte_20['day_X_has_order'] = np.where(
    df_corte_20['num_days_with_order'] == 0, 0,
    df_corte_20['num_days_with_order'] / df_corte_20['day']
)

df_corte_30['day_X_has_order'] = np.where(
    df_corte_30['num_days_with_order'] == 0, 0,
    df_corte_30['num_days_with_order'] / df_corte_30['day']
)

df_corte_10.head(10)

df_corte_20.head(10)

df_corte_30.head(10)

def plotar_correlacao_features(df_features, metodo='pearson', figsize=(20, 15)):
    """
    Plots a heatmap of the correlation matrix between numerical features.
    (Plota um gráfico de calor da matriz de correlação entre variáveis numéricas.)

    Parameters:
        df_features (pd.DataFrame): DataFrame containing only features
                                   (DataFrame contendo apenas as features)
        metodo (str): Correlation method: 'pearson', 'spearman', or 'kendall'
                     (Método de correlação: 'pearson', 'spearman' ou 'kendall')
        figsize (tuple): Size of the heatmap figure
                        (Tamanho da figura do gráfico de calor)

    Returns:
        None
    """
    # Select only numeric columns
    # (Seleciona apenas colunas numéricas)
    df_num = df_features.select_dtypes(include=['number'])

    # Compute correlation matrix
    # (Calcula a matriz de correlação)
    corr = df_num.corr(method=metodo)

    # Plot heatmap
    # (Plota o gráfico de calor)
    plt.figure(figsize=figsize)
    sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=0.5, cbar_kws={'shrink': 0.8})
    plt.title(f'Feature Correlation Heatmap ({metodo.capitalize()})')  # (Título do gráfico)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

"""Corr: Several pairs of features show strong correlations. Although some may be redundant, we will keep all of them for now and allow the model (or future feature selection) to decide which are most predictive (Vários pares de variáveis mostram correlação forte. Embora algumas possam ser redundantes, manteremos todas por enquanto e deixaremos que o modelo (ou seleção de variáveis futura) identifique as mais preditivas)."""

plotar_correlacao_features(df_corte_10, metodo='pearson')

plotar_correlacao_features(df_corte_20, metodo='pearson')

plotar_correlacao_features(df_corte_30, metodo='pearson')

"""# 9. Standardization or Normalization
* Some features like may vary in scale and range. For some machine learning models (e.g., KNN, SVM, neural networks), this can impact performance. (Algumas variáveis podem variar bastante em escala e amplitude. Para alguns modelos de machine learning (como KNN, SVM, redes neurais), isso pode afetar o desempenho.

1. Skewness and Kurtosis as Transformation Criteria:
* Skewness measures the asymmetry of the distribution. A skewness > 1 or < -1 suggests strong asymmetry and the need for transformation (e.g., log, sqrt) (Assimetria mede o quão desigual é a distribuição. Valores > 1 ou < -1 indicam forte assimetria e sugerem a necessidade de transformação (como log ou raiz).
* Kurtosis measures the "tailedness" of the distribution. High kurtosis (> 3) indicates heavy tails and potential outliers. When both are far from normal (skew ≠ 0, kurtosis ≠ 3), applying transformations can stabilize variance and make the distribution closer to Gaussian (Curtose  mede o peso das caudas da distribuição. Curtose alta (> 3) indica caudas pesadas e possível presença de outliers)
## When both are far from normal (skew ≠ 0, kurtosis ≠ 3), applying transformations can stabilize variance and make the distribution closer to Gaussian (Quando ambas se afastam da normalidade (assimetria ≠ 0, curtose ≠ 3), aplicar transformações pode estabilizar a variância e aproximar a distribuição de uma Gaussiana)
"""

df_corte_10.columns

df_corte_10.info()

"""## Cut 10
* **total_transaction_amount → QuantileTransformer**
* has_order → No
* qtd_orders → PowerTransformer
* total_orders_in_month → PowerTransformer
* num_days_with_order → PowerTransformer
* cutoff_day_if_order → PowerTransformer
* avg_ticket_per_day → PowerTransformer
* **avg_ticket_per_cutoff → QuantileTransformer**
* conversion_rate_10_days → PowerTransformer
* recency_score → No
* p_future_order → PowerTransformer
* avg_orders_per_order_day → PowerTransformer
* p_binom → PowerTransformer
* p_at_least_5_orders_binom → PowerTransformer
* num_days_with_order_X_cutoff_day_if_order → PowerTransformer
* total_transaction_amount_X_cutoff_day_if_order → PowerTransformer
* day_X_has_order → PowerTransformer
"""

# 1. Select numerical variables from your dataset
# (Seleciona variáveis numéricas do seu dataset)
var = ['total_transaction_amount', 'has_order', 'qtd_orders',
       'total_orders_in_month', 'num_days_with_order', 'cutoff_day_if_order',
       'avg_ticket_per_day', 'avg_ticket_per_cutoff', 'conversion_rate_10_days',
       'recency_score', 'remaining_days', 'p_future_order',
       'avg_orders_per_order_day', 'p_binom','p_at_least_5_orders_binom',
       'num_days_with_order_X_cutoff_day_if_order','total_transaction_amount_X_cutoff_day_if_order', 'day_X_has_order'
]

# 2. Function to apply and visualize transformations
# (Função para aplicar e visualizar transformações)
def analisar_transformacoes(df, var):
    print(f"\n📌 Variable (Variável): {var}")
    y = df[var].dropna()

    # Transformations
    # (Transformações)
    logsim_y = np.sign(y) * np.log1p(np.abs(y))  # log with sign (log com sinal)
    cbrt_y = np.cbrt(y)                          # cube root (raiz cúbica)

    power = PowerTransformer(method='yeo-johnson')  # Yeo-Johnson transformation
    power_y = power.fit_transform(y.values.reshape(-1, 1)).ravel()

    quantile = QuantileTransformer(output_distribution='normal', random_state=83)
    quantile_y = quantile.fit_transform(y.values.reshape(-1, 1)).ravel()

    # Statistics summary
    # (Resumo das estatísticas)
    def estatisticas(nome, vetor):
        print(f"{nome:<30} →  skew: {skew(vetor):>6.2f}  |  kurtosis: {kurtosis(vetor):>6.2f}")

    estatisticas("Original", y)
    estatisticas("Log with sign", logsim_y)
    estatisticas("Cube root", cbrt_y)
    estatisticas("PowerTransformer (YJ)", power_y)
    estatisticas("QuantileTransformer", quantile_y)

    # Histogram plots for each transformation
    # (Plota histogramas para cada transformação)
    fig, axs = plt.subplots(3, 2, figsize=(14, 12))
    sns.histplot(y, bins=60, kde=True, ax=axs[0, 0])
    axs[0, 0].set_title('Original')

    sns.histplot(logsim_y, bins=60, kde=True, ax=axs[0, 1])
    axs[0, 1].set_title('Log with sign (Log com sinal)')

    sns.histplot(cbrt_y, bins=60, kde=True, ax=axs[1, 0])
    axs[1, 0].set_title('Cube root (Raiz cúbica)')

    sns.histplot(power_y, bins=60, kde=True, ax=axs[1, 1])
    axs[1, 1].set_title('PowerTransformer (Yeo-Johnson)')

    sns.histplot(quantile_y, bins=60, kde=True, ax=axs[2, 0])
    axs[2, 0].set_title('QuantileTransformer (Normal)')

    axs[2, 1].axis('off')  # empty plot space (espaço vazio)

    plt.suptitle(f'Transformations: {var} (Transformações)', fontsize=16)
    plt.tight_layout()
    plt.show()

# 3. Apply to each selected variable
# (Aplica para cada variável selecionada)
for var in var:
    analisar_transformacoes(df_corte_10, var)

"""##Cut 20
* **total_transaction_amount → QuantileTransformer**
* has_order → No
* qtd_orders → PowerTransformer
* total_orders_in_month → PowerTransformer
* num_days_with_order → PowerTransformer
* cutoff_day_if_order → PowerTransformer
* avg_ticket_per_day → PowerTransformer
* **avg_ticket_per_cutoff → QuantileTransformer**
* conversion_rate_10_days → PowerTransformer
* recency_score → No
* p_future_order → PowerTransformer
* avg_orders_per_order_day → PowerTransformer
* p_binom → PowerTransformer
* p_at_least_5_orders_binom → PowerTransformer
* num_days_with_order_X_cutoff_day_if_order → PowerTransformer
* total_transaction_amount_X_cutoff_day_if_order → PowerTransformer
* day_X_has_order → PowerTransformer
"""

# Select numerical variables from your dataset
# (Seleciona variáveis numéricas do seu dataset)
var = ['total_transaction_amount', 'has_order', 'qtd_orders',
       'total_orders_in_month', 'num_days_with_order', 'cutoff_day_if_order',
       'avg_ticket_per_day', 'avg_ticket_per_cutoff', 'conversion_rate_10_days',
       'recency_score', 'remaining_days', 'p_future_order',
       'avg_orders_per_order_day', 'p_binom','p_at_least_5_orders_binom',
       'num_days_with_order_X_cutoff_day_if_order','total_transaction_amount_X_cutoff_day_if_order', 'day_X_has_order'
]

for var in var:
    analisar_transformacoes(df_corte_20, var)

"""## Cut 30
* total_transaction_amount → QuantileTransformer
* has_order → No
* qtd_orders → PowerTransformer
* total_orders_in_month → PowerTransformer
* num_days_with_order → PowerTransformer
* cutoff_day_if_order → PowerTransformer
* avg_ticket_per_day → PowerTransformer
* **avg_ticket_per_cutoff → QuantileTransformer**
* conversion_rate_10_days → PowerTransformer
* recency_score → PowerTransformer
* p_future_order → -
* avg_orders_per_order_day → PowerTransformer
* p_binom → PowerTransformer
* p_at_least_5_orders_binom → PowerTransformer
* num_days_with_order_X_cutoff_day_if_order → PowerTransformer
* total_transaction_amount_X_cutoff_day_if_order → QuantileTransformer
* day_X_has_order → PowerTransformer
"""

# Select numerical variables from your dataset
# (Seleciona variáveis numéricas do seu dataset)
var = ['total_transaction_amount', 'has_order', 'qtd_orders',
       'total_orders_in_month', 'num_days_with_order', 'cutoff_day_if_order',
       'avg_ticket_per_day', 'avg_ticket_per_cutoff', 'conversion_rate_10_days',
       'recency_score', 'remaining_days', 'p_future_order',
       'avg_orders_per_order_day', 'p_binom','p_at_least_5_orders_binom',
       'num_days_with_order_X_cutoff_day_if_order','total_transaction_amount_X_cutoff_day_if_order', 'day_X_has_order'
]

for var in var:
    analisar_transformacoes(df_corte_30, var)

"""# 10.1. Train and Test Split - Cut 10"""

# Count total observations per month (Conta o total de observações por mês)
monthly_counts = df_hist_end['year_month'].value_counts().sort_index()

# Calculate proportions over total (Calcula a proporção de cada mês sobre o total)
monthly_proportions = (monthly_counts / monthly_counts.sum()).round(4)  # 4 casas decimais (4 decimal places)

# Combine both into a DataFrame (Combina ambos em um DataFrame)
monthly_summary = pd.DataFrame({
    'num_observations': monthly_counts,
    'proportion': monthly_proportions
}).reset_index().rename(columns={'index': 'year_month'})

# Show the result (Mostra o resultado)
print(monthly_summary)

df_corte_10.columns

df_corte_10.info()

target = 'total_orders_in_month' #PowerTransformer (YJ)
features = [
            'year_month','year', 'month', 'day', 'days_of_the_week',     #date
            'is_weekend', 'is_holiday', 'is_business_day',  #bool
            'total_transaction_amount', 'has_order', 'qtd_orders',
            'num_days_with_order', 'cutoff_day_if_order', 'avg_ticket_per_day',
            'avg_ticket_per_cutoff',  'conversion_rate_10_days', 'recency_score',
            'remaining_days', 'p_future_order', 'avg_orders_per_order_day',
            'p_binom', 'p_at_least_5_orders_binom', 'num_days_with_order_X_cutoff_day_if_order',
            'total_transaction_amount_X_cutoff_day_if_order', 'day_X_has_order'
]

df_model_10 = df_corte_10[features + [target]].copy()

df_model_10.head()

# Variables based on date information to be scaled
# (Variáveis baseadas em data que devem ser padronizadas)
date_vars_10 = ['year', 'month', 'day', 'days_of_the_week']

# Boolean/categorical variables to be one-hot encoded
# (Variáveis booleanas/categóricas que devem ser transformadas com OneHotEncoder)
bool_vars_10 = ['is_weekend', 'is_holiday', 'is_business_day']

# Variables to be transformed with QuantileTransformer
# (Variáveis que devem ser transformadas com QuantileTransformer)
quantile_vars_10 = ['total_transaction_amount', 'avg_ticket_per_cutoff']

# Variables to be transformed with PowerTransformer (Yeo-Johnson)
# (Variáveis que devem ser transformadas com PowerTransformer (Yeo-Johnson))
power_vars_10 = [
    'qtd_orders',
    'num_days_with_order',
    'cutoff_day_if_order',
    'avg_ticket_per_day',
    'conversion_rate_10_days',
    'p_future_order',
    'avg_orders_per_order_day',
    'p_binom',
    'p_at_least_5_orders_binom',
    'num_days_with_order_X_cutoff_day_if_order',
    'total_transaction_amount_X_cutoff_day_if_order',
    'day_X_has_order'
]

# Variables that do not need transformation
# (Variáveis que não precisam de transformação)
no_transform_vars_10 = ['has_order', 'recency_score']

df_model_10['year_month'] = pd.to_datetime(df_model_10['year_month'])

# Target name
# (Nome da variável alvo)
target = target

# Features usadas no modelo
# (Features used in the model)
features_model = features  # sua lista anterior

# Define training and testing months
# (Definir meses de treino e teste)
train_months = [
    '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06',
    '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12',
    '2022-01', '2022-02', '2022-03', '2022-04', '2022-05'
]
test_months = ['2022-06', '2022-07']

# Separar X e y para treino e teste do corte 10
# (Separate X and y for cutoff 10)
df_corte_train_10 = df_model_10[df_model_10['year_month'].dt.strftime('%Y-%m').isin(train_months)].copy()
df_corte_test_10 = df_model_10[df_model_10['year_month'].dt.strftime('%Y-%m').isin(test_months)].copy()

# (Split X and y)
X_train_10 = df_corte_train_10[features_model].copy()
y_train_10_raw = df_corte_train_10[[target]].values

X_test_10 = df_corte_test_10[features_model].copy()
y_test_10_raw = df_corte_test_10[[target]].values


# Initialize PowerTransformer for the target variable
# (Inicializa o PowerTransformer para o target)
pt = PowerTransformer(method='yeo-johnson', standardize=True)

# Fit and transform target on training set
# (Ajusta e transforma o target no treino)
y_train_10 = pt.fit_transform(y_train_10_raw)

# Apply the transformation on test set - no fit
# (Aplica a transformação no teste (sem fit))
y_test_10 = pt.transform(y_test_10_raw)

# (Drop the date column after splitting)
X_train_10.drop(columns=['year_month'], inplace=True)
X_test_10.drop(columns=['year_month'], inplace=True)

X_train_10.head()

y_train_10

# Full transformation pipeline
# (Pipeline de transformação completo)

# 1. Categorical: only OneHotEncoder
# (Categóricas: apenas OneHotEncoder)
cat_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

# 2. Numerical: only StandardScaler
# (Numéricas comuns: apenas StandardScaler)
num_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# 3. QuantileTransformer for selected variables
# (QuantileTransformer para variáveis selecionadas)
quantile_transformer = Pipeline(steps=[
    ('quantile', QuantileTransformer(output_distribution='normal', n_quantiles=100, random_state=42))
])

# 4. PowerTransformer for other selected variables
# (PowerTransformer (Yeo-Johnson) para outras variáveis)
power_transformer = Pipeline(steps=[
    ('power', PowerTransformer(method='yeo-johnson', standardize=True))
])

# 5. No transformation
# (Nenhuma transformação)
no_transform = 'passthrough'

# Building the full ColumnTransformer
# (Construindo o ColumnTransformer completo)
preprocessor_10 = ColumnTransformer(transformers=[
    ('cat', cat_transformer, bool_vars_10),              # OneHotEncoder para booleanas
    ('num', num_transformer, date_vars_10),              # StandardScaler para datas
    ('quant', quantile_transformer, quantile_vars_10),   # QuantileTransformer
    ('power', power_transformer, power_vars_10),         # PowerTransformer
    ('no_transform', no_transform, no_transform_vars_10) # Sem transformação
])

"""# 11.1 Model training - Cut 10

1. Evaluation Metrics – Métricas de Avaliação
* MAE – Mean Absolute Error (Erro Absoluto Médio): Measures the average absolute difference between predictions and actual values (Mede a diferença média absoluta entre previsões e valores reais.)
* Shows how much, on average, the model is off from the true value (Mostra quanto, em média, o modelo erra em relação ao valor real).
* RMSE – Root Mean Squared Error (Raiz do Erro Quadrático Médio): Measures the average error after squaring the differences before summing (Mede o erro médio elevando ao quadrado as diferenças antes de somar).
* Penalizes large errors more strongly than MAE (Penaliza mais fortemente os erros grandes do que o MAE).
* R² – Coefficient of Determination (Coeficiente de Determinação): Measures how much of the variance in the target variable is explained by the model (Mede quanto da variância da variável alvo é explicada pelo modelo):
    a. R² = 1: perfect model (R² = 1: modelo perfeito);
    b. R² ≤ 0: model explains nothing (R² ≤ 0: o modelo não explica nada).
* Comparing R² in train and test helps detect overfitting (A comparação do R² entre treino e teste ajuda a detectar overfitting).

2. Overfitting and Underfitting – Sobreajuste e Subajuste
* Overfitting (Sobreajuste): The model learns the training data too well, including noise (O modelo aprende muito bem os dados de treino, inclusive os ruídos).
* R² is very high in training and low in testing (R² muito alto no treino e baixo no teste).
* Large difference between train and test R² (Grande diferença entre o R² de treino e teste).
*  Low train error, high test error (Erro baixo no treino, erro alto no teste).

3. Underfitting (Subajuste):
* The model is too simple and fails to capture the data structure (O modelo é muito simples e não consegue capturar a estrutura dos dados).
* Low R² in both training and testing (R² baixo tanto no treino quanto no teste).
* High errors in both datasets (Erros altos em ambos os conjuntos).

4. Dif_R2 – r2_score(y_train_real, y_train_pred) - r2_score(y_real, y_pred):
* Measures the gap between training and test R² (Mede a diferença entre o R² de treino e de teste).
* Helps identify overfitting or underfitting behavior (Ajuda a identificar comportamento de overfitting ou underfitting).


"""

# Modelos a serem avaliados
modelos = {
    'LinearRegression': LinearRegression(),
    'RandomForest': RandomForestRegressor(random_state=42)
    }

# Modelos com ajuste pesado de parâmetros (usar RandomizedSearch)
pesados = ['RandomForest']

# Hiperparâmetros
parametros = {
    'RandomForest': {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__max_depth': [5, 7, 10],
        'regressor__min_samples_split': [2, 5],
        'regressor__min_samples_leaf': [1, 3],
        'regressor__max_features': ['sqrt']
    }
}

# Initialize results and error lists
# (Inicializa listas de resultados e erros)
resultados_10 = []
modelos_com_erros_10 = []

# --- Loop over models (Percorre os modelos)
for nome, modelo in modelos.items():
    print(f"\n🚀 Treinando: {nome}...")

    try:
        pipe = Pipeline(steps=[
            ('preprocessamento', preprocessor_10),  # ✅ Nome corrigido
            ('regressor', modelo)
        ])

        # Verifica se o modelo tem hiperparâmetros (Check if the model has hyperparameters)
        if nome in parametros:
            if nome in pesados:
                # RandomizedSearchCV para modelos pesados (RandomizedSearchCV for heavy models)
                search = RandomizedSearchCV(
                    estimator=pipe,
                    param_distributions=parametros[nome],
                    scoring='neg_root_mean_squared_error',
                    cv=5,
                    n_jobs=1,
                    n_iter=5,
                    verbose=2,
                    random_state=42,
                    error_score='raise'
                )
            else:
                # GridSearchCV para modelos leves (GridSearchCV for lightweight models)
                search = GridSearchCV(
                    estimator=pipe,
                    param_grid=parametros[nome],
                    scoring='neg_root_mean_squared_error',
                    cv=5,
                    n_jobs=1,
                    verbose=1
                )

            # Ajusta modelo com validação cruzada (Fit model with cross-validation)
            search.fit(X_train_10, y_train_10)
            best_model = search.best_estimator_
            y_pred_trans = best_model.predict(X_test_10)
            y_train_pred_trans = best_model.predict(X_train_10)

            print(f"✅ Melhores parâmetros para {nome}: {search.best_params_}")

        else:
            # Ajuste direto sem tuning (Direct fitting without tuning)
            pipe.fit(X_train_10, y_train_10)
            best_model = pipe
            y_pred_trans = pipe.predict(X_test_10)
            y_train_pred_trans = pipe.predict(X_train_10)

        # Reversão do target com PowerTransformer (Inverse target transformation)
        y_real = pt.inverse_transform(y_test_10).ravel()
        y_pred = pt.inverse_transform(y_pred_trans.reshape(-1, 1)).ravel()
        y_train_real = pt.inverse_transform(y_train_10).ravel()
        y_train_pred = pt.inverse_transform(y_train_pred_trans.reshape(-1, 1)).ravel()

        # Métricas e comparação (Metrics and comparison)
        resultados_10.append({
            'modelo': nome,
            'MAE': mean_absolute_error(y_real, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_real, y_pred)),
            'R2_Teste': r2_score(y_real, y_pred),
            'R2_Treino': r2_score(y_train_real, y_train_pred),
            'dif_R2': r2_score(y_train_real, y_train_pred) - r2_score(y_real, y_pred)
        })

        print(f"✅ Concluído: {nome}")

    except Exception as e:
        print(f"❌ Erro ao treinar {nome}: {e}")
        modelos_com_erros_10.append((nome, str(e)))

"""## RandomForest
Best overall performance with balanced generalization and minimal overfitting.
(Melhor desempenho geral com boa generalização e baixo overfitting.)

RandomForest: {'regressor__n_estimators': 200, 'regressor__min_samples_split': 5, 'regressor__min_samples_leaf': 3, 'regressor__max_features': 'sqrt', 'regressor__max_depth': 10}

"""

# ✅ Exibir ranking dos modelos do corte 10 (Show model ranking for cutoff 10)
if resultados_10:
    df_resultados_10 = pd.DataFrame(resultados_10)
    df_resultados_10 = df_resultados_10.sort_values(by='RMSE')

    print("\n📊 Ranking de Modelos - Corte 10:")
    display(df_resultados_10)
else:
    print("❌ Nenhum modelo gerou resultados válidos para o corte 10.")
    if modelos_com_erros_10:
        print("\n📋 Modelos com erro:")
        for nome, erro in modelos_com_erros_10:
            print(f"- {nome}: {erro}")

plt.figure(figsize=(8, 6))
sns.scatterplot(data=df_resultados_10, x='R2_Treino', y='R2_Teste', hue='modelo')
plt.plot([0, 1], [0, 1], '--', color='gray')  # linha de perfeição
plt.title('Treino vs Teste R²')
plt.xlabel('R² no Treino')
plt.ylabel('R² no Teste')
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib
# 1. Your final chosen hyperparameters (Seus hiperparâmetros escolhidos)
parametros_escolhidos = {
    'n_estimators': 200,
    'min_samples_split': 5,
    'min_samples_leaf': 3,
    'max_features': 'sqrt',
    'max_depth': 10
}

# 2. Pipeline with only the final model (Pipeline só com o modelo final)
modelo_final_10 = Pipeline(steps=[
    ('preprocessamento', preprocessor_10),  # ✅ Inclui transformações
    ('regressor', RandomForestRegressor(**parametros_escolhidos, random_state=42))
])

# Fit using preprocessed data (Treina com os dados já pré-processados)
modelo_final_10.fit(X_train_10, y_train_10)

# Previsões no teste
y_pred_trans = modelo_final_10.predict(X_test_10)

# Reverter o target se foi transformado (como com PowerTransformer)
y_real = pt.inverse_transform(y_test_10).ravel()
y_pred = pt.inverse_transform(y_pred_trans.reshape(-1, 1)).ravel()

print("📊 Avaliação do modelo escolhido:")
print("MAE:", mean_absolute_error(y_real, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_real, y_pred)))
print("R²:", r2_score(y_real, y_pred))

joblib.dump(modelo_final_10, 'modelo_final_10_randomforest.pkl')
print("✅ Modelo salvo como modelo_final_10_randomforest.pkl")

import os

os.path.exists('modelo_final_10_randomforest.pkl')

"""## Prediction Scatter Analysis (Análise de Dispersão das Previsões)
Graph Interpretation
(Interpretação do Gráfico)

* Red dashed line represents the ideal scenario where predictions are equal to real values. (A linha vermelha tracejada representa o cenário ideal onde os valores previstos são iguais aos reais.)
* Blue dots are actual prediction points. (Os pontos azuis são as previsões reais do modelo.)

1. Well-aligned for low values (0–5)
(Bem alinhado para valores baixos (0–5))

2. Predictions closely follow the diagonal, showing low error.
(As previsões seguem de perto a linha ideal, com baixo erro.)

3. Increasing dispersion as real values increase
(Dispersão crescente conforme aumentam os valores reais)

4. The spread of predictions widens from 10 onward.
(A dispersão das previsões aumenta a partir de 10.)

5. Systematic underestimation for high values (15+)
(Subestimação sistemática para valores altos (15+))

6. Many predicted values fall below the red line.
(Muitos valores previstos ficam abaixo da linha vermelha.)

7. Prediction ceiling effect near 20
(Efeito teto nas previsões próximo a 20)

8. Even when real values approach 25, predictions rarely exceed 20.
(Mesmo quando os valores reais se aproximam de 25, as previsões raramente passam de 20.)
"""

plt.figure(figsize=(6,6))
plt.scatter(y_real, y_pred, alpha=0.3)
plt.plot([y_real.min(), y_real.max()], [y_real.min(), y_real.max()], 'r--')
plt.xlabel('Valor Real')
plt.ylabel('Valor Previsto')
plt.title('Dispersão: Valor Real vs Previsto')
plt.grid(True)
plt.tight_layout()
plt.show()

"""##Error Analysis (Análise dos Erros)
Boxplot Interpretation by Real Value Ranges
(Interpretação do Boxplot por Faixas de Valor Real)

Real Value Range (Faixa de Valor Real):		Notes (Observações)
* (0, 2]	Median Absolute Error (Mediana do Erro Absoluto): ~0.3 to 0.5.
Excellent performance where most users are concentrated. (Desempenho excelente onde a maioria dos usuários está concentrada.)
* (2, 5]	Median Absolute Error (Mediana do Erro Absoluto): ~0.5 to 1.0.
Very good accuracy in the dominant range. (Muito boa precisão na faixa dominante.)
* (5, 10]	Median Absolute Error (Mediana do Erro Absoluto): ~1.5.
Error starts to grow but remains acceptable. (Erro começa a crescer, mas ainda aceitável.)
* (10, 15]	Median Absolute Error (Mediana do Erro Absoluto): ~2.5.
Error increases, requiring more attention. (Erro aumenta, exigindo mais atenção.)
* (15, 20]	Median Absolute Error (Mediana do Erro Absoluto): ~3.5 to 4.0.
High absolute error; may require dedicated treatment. (Erro absoluto alto; pode exigir tratamento dedicado.)
* (20, 26]	Median Absolute Error (Mediana do Erro Absoluto): ~5 to 6.
High error and dispersion; weak performance on outliers. (Erro alto e dispersão grande; desempenho fraco nos extremos.)
"""

# Error by true value bin
# (Erro por faixa do valor real)
df_erro = pd.DataFrame({
    'y_real': y_real,
    'y_pred': y_pred
})
df_erro['erro_absoluto'] = abs(df_erro['y_real'] - df_erro['y_pred'])
df_erro['faixa'] = pd.cut(df_erro['y_real'], bins=[0, 2, 5, 10, 15, 20, 26])

plt.figure(figsize=(10, 4))
sns.boxplot(data=df_erro, x='faixa', y='erro_absoluto')
plt.title('Erro absoluto por faixa de valor real')
plt.ylabel('Erro Absoluto')
plt.xlabel('Faixa de Valor Real')
plt.grid(True)
plt.show()

"""## 10.2. Train and Test Split - Cut 20"""

df_corte_20.columns

df_corte_20.info()

target = 'total_orders_in_month' #PowerTransformer (YJ)
features = [
            'year_month','year', 'month', 'day', 'days_of_the_week',     #date
            'is_weekend', 'is_holiday', 'is_business_day',               #bool
            'total_transaction_amount', 'has_order', 'qtd_orders',
            'num_days_with_order', 'cutoff_day_if_order', 'avg_ticket_per_day',
            'avg_ticket_per_cutoff', 'conversion_rate_10_days', 'recency_score',
            'remaining_days', 'p_future_order', 'avg_orders_per_order_day',
            'p_binom', 'p_at_least_5_orders_binom', 'num_days_with_order_X_cutoff_day_if_order',
            'total_transaction_amount_X_cutoff_day_if_order', 'day_X_has_order'
]

df_model_20 = df_corte_20[features + [target]].copy()

df_model_20.head()

# Variables based on date information to be scaled
# (Variáveis baseadas em data que devem ser padronizadas)
date_vars_20 = ['year', 'month', 'day', 'days_of_the_week']

# Boolean/categorical variables to be one-hot encoded
# (Variáveis booleanas/categóricas que devem ser transformadas com OneHotEncoder)
bool_vars_20 = ['is_weekend', 'is_holiday', 'is_business_day']

# Variables to be transformed with QuantileTransformer
# (Variáveis que devem ser transformadas com QuantileTransformer)
quantile_vars_20 = ['total_transaction_amount', 'avg_ticket_per_cutoff']

# Variables to be transformed with PowerTransformer (Yeo-Johnson)
# (Variáveis que devem ser transformadas com PowerTransformer (Yeo-Johnson))
power_vars_20 = [
    'qtd_orders',
    'num_days_with_order',
    'cutoff_day_if_order',
    'avg_ticket_per_day',
    'conversion_rate_10_days',
    'p_future_order',
    'avg_orders_per_order_day',
    'p_binom',
    'p_at_least_5_orders_binom',
    'num_days_with_order_X_cutoff_day_if_order',
    'total_transaction_amount_X_cutoff_day_if_order',
    'day_X_has_order'
]

# Variables that do not need transformation
# (Variáveis que não precisam de transformação)
no_transform_vars_20 = ['has_order', 'recency_score']

df_model_20['year_month'] = pd.to_datetime(df_model_20['year_month'])

# Target name
# (Nome da variável alvo)
target = target

# Features usadas no modelo
# (Features used in the model)
features_model = features  # sua lista anterior

# Define training and testing months
# (Definir meses de treino e teste)
train_months = [
    '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06',
    '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12',
    '2022-01', '2022-02', '2022-03', '2022-04', '2022-05'
]
test_months = ['2022-06', '2022-07']

# Separar X e y para treino e teste do corte 10
# (Separate X and y for cutoff 10)
df_corte_train_20 = df_model_20[df_model_20['year_month'].dt.strftime('%Y-%m').isin(train_months)].copy()
df_corte_test_20 = df_model_20[df_model_20['year_month'].dt.strftime('%Y-%m').isin(test_months)].copy()

# (Split X and y)
X_train_20 = df_corte_train_20[features_model].copy()
y_train_20_raw = df_corte_train_20[[target]].values

X_test_20 = df_corte_test_20[features_model].copy()
y_test_20_raw = df_corte_test_20[[target]].values


# Initialize PowerTransformer for the target variable
# (Inicializa o PowerTransformer para o target)
pt = PowerTransformer(method='yeo-johnson', standardize=True)

# Fit and transform target on training set
# (Ajusta e transforma o target no treino)
y_train_20 = pt.fit_transform(y_train_20_raw)

# Apply the transformation on test set - no fit
# (Aplica a transformação no teste (sem fit))
y_test_20 = pt.transform(y_test_20_raw)

# (Drop the date column after splitting)
X_train_20.drop(columns=['year_month'], inplace=True)
X_test_20.drop(columns=['year_month'], inplace=True)

X_train_20.head()

X_train_20.info()

y_train_20

# Full transformation pipeline
# (Pipeline de transformação completo)

# 1. Categorical: only OneHotEncoder
# (Categóricas: apenas OneHotEncoder)
cat_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

# 2. Numerical: only StandardScaler
# (Numéricas comuns: apenas StandardScaler)
num_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# 3. QuantileTransformer for selected variables
# (QuantileTransformer para variáveis selecionadas)
quantile_transformer = Pipeline(steps=[
    ('quantile', QuantileTransformer(output_distribution='normal', n_quantiles=100, random_state=42))
])

# 4. PowerTransformer for other selected variables
# (PowerTransformer (Yeo-Johnson) para outras variáveis)
power_transformer = Pipeline(steps=[
    ('power', PowerTransformer(method='yeo-johnson', standardize=True))
])

# 5. No transformation
# (Nenhuma transformação)
no_transform = 'passthrough'

# Building the full ColumnTransformer
# (Construindo o ColumnTransformer completo)
preprocessor_20 = ColumnTransformer(transformers=[
    ('cat', cat_transformer, bool_vars_20),              # OneHotEncoder para booleanas
    ('num', num_transformer, date_vars_20),              # StandardScaler para datas
    ('quant', quantile_transformer, quantile_vars_20),   # QuantileTransformer
    ('power', power_transformer, power_vars_20),         # PowerTransformer
    ('no_transform', no_transform, no_transform_vars_20) # Sem transformação
])

"""## 11.2. Model training - Cut 20"""

# Models
modelos = {
    'LinearRegression': LinearRegression(),
    'RandomForest': RandomForestRegressor(random_state=42)
}

# RandomizedSearch
pesados = ['RandomForest']
# Hyperparameters
parametros = {

    'RandomForest': {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__max_depth': [5, 7, 10],
        'regressor__min_samples_split': [2, 5],
        'regressor__min_samples_leaf': [1, 3],
        'regressor__max_features': ['sqrt']
    }
}

# Initialize results and error lists
# (Inicializa listas de resultados e erros)
resultados_20 = []
modelos_com_erros_20 = []

# --- Loop over models (Percorre os modelos)
for nome, modelo in modelos.items():
    print(f"\n🚀 Treinando: {nome}...")

    try:
        pipe = Pipeline(steps=[
            ('preprocessamento', preprocessor_20),
            ('regressor', modelo)
        ])

        # Check if the model has hyperparameters (Verifica se o modelo tem hiperparâmetros)
        if nome in parametros:
            if nome in pesados:
                # RandomizedSearchCV for heavy models (RandomizedSearchCV para modelos pesados)
                search = RandomizedSearchCV(
                    estimator=pipe,
                    param_distributions=parametros[nome],
                    scoring='neg_root_mean_squared_error',
                    cv=5,
                    n_jobs=1,
                    n_iter=5,
                    verbose=2,
                    random_state=42,
                    error_score='raise'
                )
            else:
                # GridSearchCV for lightweight models (GridSearchCV para modelos leves)
                search = GridSearchCV(
                    estimator=pipe,
                    param_grid=parametros[nome],
                    scoring='neg_root_mean_squared_error',
                    cv=5,
                    n_jobs=1,
                    verbose=1
                )

            # Fit model with cross-validation (Ajusta modelo com validação cruzada)
            search.fit(X_train_20, y_train_20)
            best_model = search.best_estimator_
            y_pred_trans = best_model.predict(X_test_20)
            y_train_pred_trans = best_model.predict(X_train_20)

            print(f"✅ Melhores parâmetros para {nome}: {search.best_params_}")

        else:
            # Direct fitting without tuning (Ajuste direto sem tuning)
            pipe.fit(X_train_20, y_train_20)
            best_model = pipe
            y_pred_trans = pipe.predict(X_test_20)
            y_train_pred_trans = pipe.predict(X_train_20)

        # Inverse target transformation (Reversão do target com PowerTransformer)
        y_real = pt.inverse_transform(y_test_20).ravel()
        y_pred = pt.inverse_transform(y_pred_trans.reshape(-1, 1)).ravel()
        y_train_real = pt.inverse_transform(y_train_20).ravel()
        y_train_pred = pt.inverse_transform(y_train_pred_trans.reshape(-1, 1)).ravel()

        # Metrics and comparison (Métricas e comparação)
        resultados_20.append({
            'modelo': nome,
            'MAE': mean_absolute_error(y_real, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_real, y_pred)),
            'R2_Teste': r2_score(y_real, y_pred),
            'R2_Treino': r2_score(y_train_real, y_train_pred),
            'dif_R2': r2_score(y_train_real, y_train_pred) - r2_score(y_real, y_pred)
        })

        print(f"✅ Concluído: {nome}")

    except Exception as e:
        print(f"❌ Erro ao treinar {nome}: {e}")
        modelos_com_erros_20.append((nome, str(e)))

"""## RandomForest
RandomForest achieved the best overall performance in cutoff 20, showing a relatively low MAE and RMSE, and explaining about 68.1% of the variance in test data.
(Melhor desempenho geral com boa generalização e baixo overfitting.)

RandomForest: {'regressor__n_estimators': 200, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 1, 'regressor__max_features': 'sqrt', 'regressor__max_depth': 10}
"""

# ✅ Show model ranking for cutoff 20
if resultados_20:
    df_resultados_20 = pd.DataFrame(resultados_20)
    df_resultados_20 = df_resultados_20.sort_values(by='RMSE')

    print("\n📊 Ranking de Modelos - Corte 20:")
    display(df_resultados_20)
else:
    print("❌ Nenhum modelo gerou resultados válidos para o corte 20.")
    if modelos_com_erros_20:
        print("\n📋 Modelos com erro:")
        for nome, erro in modelos_com_erros_20:
            print(f"- {nome}: {erro}")

plt.figure(figsize=(8, 6))
sns.scatterplot(data=df_resultados_20, x='R2_Treino', y='R2_Teste', hue='modelo')
plt.plot([0, 1], [0, 1], '--', color='gray')  # linha de perfeição
plt.title('Treino vs Teste R²')
plt.xlabel('R² no Treino')
plt.ylabel('R² no Teste')
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib
# 1. Your final chosen hyperparameters (Seus hiperparâmetros escolhidos)
parametros_escolhidos = {
  'n_estimators': 200,
  'min_samples_split': 2,
  'min_samples_leaf': 1,
  'max_features': 'sqrt',
  'max_depth': 10
}

# 2. Pipeline with only the final model (Pipeline só com o modelo final)
modelo_final_20 = Pipeline(steps=[
    ('preprocessamento', preprocessor_20),  # ✅ Inclui transformações
    ('regressor', RandomForestRegressor(**parametros_escolhidos, random_state=42))
])

# 3. Fit using preprocessed data (Treina com os dados já pré-processados)
modelo_final_20.fit(X_train_20, y_train_20)

# Previsões no teste
y_pred_trans = modelo_final_20.predict(X_test_20)

# Reverter o target se foi transformado (PowerTransformer)
y_real = pt.inverse_transform(y_test_20).ravel()
y_pred = pt.inverse_transform(y_pred_trans.reshape(-1, 1)).ravel()

print("📊 Avaliação do modelo escolhido:")
print("MAE:", mean_absolute_error(y_real, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_real, y_pred)))
print("R²:", r2_score(y_real, y_pred))

joblib.dump(modelo_final_20, 'modelo_final_20_randomforest.pkl')
print("✅ Modelo salvo como modelo_final_20_randomforest.pkl")

import os

os.path.exists('modelo_final_20_randomforest.pkl')

"""## Prediction Scatter Analysis (Análise de Dispersão das Previsões)
Graph Interpretation
(Interpretação do Gráfico)

* Red dashed line represents the ideal scenario where predictions are equal to real values. (A linha vermelha tracejada representa o cenário ideal onde os valores previstos são iguais aos reais.)
* Blue dots are actual prediction points. (Os pontos azuis são as previsões reais do modelo.)

1. As the real value increases (right side of the plot), the spread of the points increases.
(À medida que o valor real aumenta (lado direito do gráfico), a dispersão dos pontos também aumenta.)

2. The model tends to underestimate higher values, as many points fall below the red line in that region.
(O modelo tende a subestimar valores altos, pois muitos pontos caem abaixo da linha vermelha nessa região.)

3. The model has good predictive behavior for low and medium ranges, but its performance decreases for higher real values, where the predictions are more dispersed and tend to be lower than they should.
(O modelo apresenta um bom comportamento preditivo para faixas baixas e médias, mas sua performance diminui para valores reais mais altos, onde as previsões são mais dispersas e tendem a ficar abaixo do valor correto.)
"""

plt.figure(figsize=(6,6))
plt.scatter(y_real, y_pred, alpha=0.3)
plt.plot([y_real.min(), y_real.max()], [y_real.min(), y_real.max()], 'r--')
plt.xlabel('Valor Real')
plt.ylabel('Valor Previsto')
plt.title('Dispersão: Valor Real vs Previsto')
plt.grid(True)
plt.tight_layout()
plt.show()

"""##Error Analysis (Análise dos Erros)
Boxplot Interpretation by Real Value Ranges
(Interpretação do Boxplot por Faixas de Valor Real)

* [0, 2] and (2, 5] ranges have the lowest absolute errors, with most predictions close to the true values.
(As faixas [0, 2] e (2, 5] apresentam os menores erros absolutos, com a maioria das previsões próximas aos valores reais.)

* From (5, 10] onwards, the median error starts increasing, and the spread widens significantly.
(A partir da faixa (5, 10], a mediana do erro começa a aumentar, e a dispersão se amplia significativamente.)

* Outliers become more frequent in higher ranges, especially beyond 10.
(Outliers tornam-se mais frequentes nas faixas mais altas, especialmente acima de 10.)

* The range (20, 26], which contains the highest values, shows the largest absolute errors.
(A faixa (20, 26], que contém os valores mais altos, apresenta os maiores erros absolutos.)

* The model performs consistently well for lower target values, but its accuracy declines as the true value increases.
(**O modelo apresenta bom desempenho para valores baixos, mas sua acurácia declina à medida que o valor real aumenta.)

* This pattern suggests that the model may be struggling to generalize for customers with higher demand or volume, potentially due to their lower frequency or more complex behavior.
(Esse padrão sugere que o modelo pode estar tendo dificuldade em generalizar para clientes com maior demanda ou volume, possivelmente devido à menor frequência ou comportamento mais complexo desses casos.)
"""

# Error by true value bin
df_erro = pd.DataFrame({
    'y_real': y_real,
    'y_pred': y_pred
})
df_erro['erro_absoluto'] = abs(df_erro['y_real'] - df_erro['y_pred'])
df_erro['faixa'] = pd.cut(df_erro['y_real'], bins=[0, 2, 5, 10, 15, 20, 26])

plt.figure(figsize=(10, 4))
sns.boxplot(data=df_erro, x='faixa', y='erro_absoluto')
plt.title('Erro absoluto por faixa de valor real')
plt.ylabel('Erro Absoluto')
plt.xlabel('Faixa de Valor Real')
plt.grid(True)
plt.show()

"""## 10.3. Train and Test Split - Cut 30"""

df_corte_30.info()

target = 'total_orders_in_month' #PowerTransformer (YJ)
features = [
            'year_month','year', 'month', 'day', 'days_of_the_week',     #date
            'is_weekend', 'is_holiday', 'is_business_day',  #bool
            'total_transaction_amount', 'has_order', 'qtd_orders',
            'num_days_with_order', 'cutoff_day_if_order', 'avg_ticket_per_day',
            'avg_ticket_per_cutoff', 'conversion_rate_10_days', 'recency_score',
            'remaining_days', 'avg_orders_per_order_day', 'p_binom',
            'p_at_least_5_orders_binom', 'num_days_with_order_X_cutoff_day_if_order',
            'total_transaction_amount_X_cutoff_day_if_order', 'day_X_has_order'
]

df_model_30 = df_corte_30[features + [target]].copy()

df_model_30.head()

# Variables based on date information to be scaled
# (Variáveis baseadas em data que devem ser padronizadas)
date_vars_30 = ['year', 'month', 'day', 'days_of_the_week']

# Boolean/categorical variables to be one-hot encoded
# (Variáveis booleanas/categóricas que devem ser transformadas com OneHotEncoder)
bool_vars_30 = ['is_weekend', 'is_holiday', 'is_business_day']

# Variables to be transformed with QuantileTransformer
# (Variáveis que devem ser transformadas com QuantileTransformer)
quantile_vars_30 = ['total_transaction_amount', 'avg_ticket_per_cutoff', 'total_transaction_amount_X_cutoff_day_if_order']

# Variables to be transformed with PowerTransformer (Yeo-Johnson)
# (Variáveis que devem ser transformadas com PowerTransformer (Yeo-Johnson))
power_vars_30 = [
    'qtd_orders',
    'num_days_with_order',
    'cutoff_day_if_order',
    'avg_ticket_per_day',
    'conversion_rate_10_days',
    'avg_orders_per_order_day',
    'p_binom',
    'p_at_least_5_orders_binom',
    'num_days_with_order_X_cutoff_day_if_order',
    'day_X_has_order',
    'recency_score'
]

# Variables that do not need transformation
# (Variáveis que não precisam de transformação)
no_transform_vars_30 = ['has_order']

df_model_30['year_month'] = pd.to_datetime(df_model_30['year_month'])

# Target name
# (Nome da variável alvo)
target = target

# Features usadas no modelo
# (Features used in the model)
features_model = features

# Define training and testing months
# (Definir meses de treino e teste)
train_months = [
    '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06',
    '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12',
    '2022-01', '2022-02', '2022-03', '2022-04', '2022-05'
]
test_months = ['2022-06', '2022-07']

# Separar X e y para treino e teste do corte 10
# (Separate X and y for cutoff 10)
df_corte_train_30 = df_model_30[df_model_30['year_month'].dt.strftime('%Y-%m').isin(train_months)].copy()
df_corte_test_30 = df_model_30[df_model_30['year_month'].dt.strftime('%Y-%m').isin(test_months)].copy()

# (Split X and y)
X_train_30 = df_corte_train_30[features_model].copy()
y_train_30_raw = df_corte_train_30[[target]].values

X_test_30 = df_corte_test_30[features_model].copy()
y_test_30_raw = df_corte_test_30[[target]].values


# Initialize PowerTransformer for the target variable
# (Inicializa o PowerTransformer para o target)
pt = PowerTransformer(method='yeo-johnson', standardize=True)

# Fit and transform target on training set
# (Ajusta e transforma o target no treino)
y_train_30 = pt.fit_transform(y_train_30_raw)

# Apply the transformation on test set - no fit
# (Aplica a transformação no teste (sem fit))
y_test_30 = pt.transform(y_test_30_raw)

# (Drop the date column after splitting)
X_train_30.drop(columns=['year_month'], inplace=True)
X_test_30.drop(columns=['year_month'], inplace=True)

X_train_30.head()

X_train_30.info()

y_train_30

# Full transformation pipeline
# (Pipeline de transformação completo)

# 1. Categorical: only OneHotEncoder
# (Categóricas: apenas OneHotEncoder)
cat_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

# 2. Numerical: only StandardScaler
# (Numéricas comuns: apenas StandardScaler)
num_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# 3. QuantileTransformer for selected variables
# (QuantileTransformer para variáveis selecionadas)
quantile_transformer = Pipeline(steps=[
    ('quantile', QuantileTransformer(output_distribution='normal', n_quantiles=100, random_state=42))
])

# 4. PowerTransformer for other selected variables
# (PowerTransformer (Yeo-Johnson) para outras variáveis)
power_transformer = Pipeline(steps=[
    ('power', PowerTransformer(method='yeo-johnson', standardize=True))
])

# 5. No transformation
# (Nenhuma transformação)
no_transform = 'passthrough'

# Building the full ColumnTransformer
# (Construindo o ColumnTransformer completo)
preprocessor_30 = ColumnTransformer(transformers=[
    ('cat', cat_transformer, bool_vars_30),              # OneHotEncoder para booleanas
    ('num', num_transformer, date_vars_30),              # StandardScaler para datas
    ('quant', quantile_transformer, quantile_vars_30),   # QuantileTransformer
    ('power', power_transformer, power_vars_30),         # PowerTransformer
    ('no_transform', no_transform, no_transform_vars_30) # Sem transformação
])

"""## 11.3. Model training - Cut 30"""

# Models
modelos = {
    'LinearRegression': LinearRegression(),
    'RandomForest': RandomForestRegressor(random_state=42)
}

# RandomizedSearch
pesados = ['RandomForest']

# Hyperparameters
parametros = {
    'RandomForest': {
        'regressor__n_estimators': [100, 200],
        'regressor__max_depth': [5, 10],
        'regressor__min_samples_split': [2, 5],
        'regressor__min_samples_leaf': [1, 3],
        'regressor__max_features': ['sqrt']
    }
}

# Initialize results and error lists
# (Inicializa listas de resultados e erros)
resultados_30 = []
modelos_com_erros_30 = []

# --- Loop over models (Percorre os modelos)
for nome, modelo in modelos.items():
    print(f"\n🚀 Treinando: {nome}...")

    try:
        pipe = Pipeline(steps=[
            ('preprocessamento', preprocessor_30),
            ('regressor', modelo)
        ])

        # Check if the model has hyperparameters (Verifica se o modelo tem hiperparâmetros)
        if nome in parametros:
            if nome in pesados:
                # RandomizedSearchCV for heavy models (RandomizedSearchCV para modelos pesados)
                search = RandomizedSearchCV(
                    estimator=pipe,
                    param_distributions=parametros[nome],
                    scoring='neg_root_mean_squared_error',
                    cv=5,
                    n_jobs=1,
                    n_iter=5,
                    verbose=2,
                    random_state=42,
                    error_score='raise'
                )
            else:
                # GridSearchCV for lightweight models (GridSearchCV para modelos leves)
                search = GridSearchCV(
                    estimator=pipe,
                    param_grid=parametros[nome],
                    scoring='neg_root_mean_squared_error',
                    cv=5,
                    n_jobs=1,
                    verbose=1
                )

            # Fit model with cross-validation (Ajusta modelo com validação cruzada)
            search.fit(X_train_30, y_train_30)
            best_model = search.best_estimator_
            y_pred_trans = best_model.predict(X_test_30)
            y_train_pred_trans = best_model.predict(X_train_30)

            print(f"✅ Melhores parâmetros para {nome}: {search.best_params_}")

        else:
            # Direct fitting without tuning (Ajuste direto sem tuning)
            pipe.fit(X_train_30, y_train_30)
            best_model = pipe
            y_pred_trans = pipe.predict(X_test_30)
            y_train_pred_trans = pipe.predict(X_train_30)

        # Inverse target transformation (Reversão do target com PowerTransformer)
        y_real = pt.inverse_transform(y_test_30).ravel()
        y_pred = pt.inverse_transform(y_pred_trans.reshape(-1, 1)).ravel()
        y_train_real = pt.inverse_transform(y_train_30).ravel()
        y_train_pred = pt.inverse_transform(y_train_pred_trans.reshape(-1, 1)).ravel()

        # Metrics and comparison (Métricas e comparação)
        resultados_30.append({
            'modelo': nome,
            'MAE': mean_absolute_error(y_real, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_real, y_pred)),
            'R2_Teste': r2_score(y_real, y_pred),
            'R2_Treino': r2_score(y_train_real, y_train_pred),
            'dif_R2': r2_score(y_train_real, y_train_pred) - r2_score(y_real, y_pred)
        })

        print(f"✅ Concluído: {nome}")

    except Exception as e:
        print(f"❌ Erro ao treinar {nome}: {e}")
        modelos_com_erros_30.append((nome, str(e)))

"""## RandomForest
Best overall performance with balanced generalization and minimal overfitting.
RandomForest: {'regressor__n_estimators': 200, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_features': 'sqrt', 'regressor__max_depth': 10}
"""

# Show model ranking for cutoff 30
if resultados_30:
    df_resultados_30 = pd.DataFrame(resultados_30)
    df_resultados_30 = df_resultados_30.sort_values(by='RMSE')

    print("\n📊 Ranking de Modelos - Corte 30:")
    display(df_resultados_30)
else:
    print("❌ Nenhum modelo gerou resultados válidos para o corte 30.")
    if modelos_com_erros_30:
        print("\n📋 Modelos com erro:")
        for nome, erro in modelos_com_erros_30:
            print(f"- {nome}: {erro}")

plt.figure(figsize=(8, 6))
sns.scatterplot(data=df_resultados_30, x='R2_Treino', y='R2_Teste', hue='modelo')
plt.plot([0, 1], [0, 1], '--', color='gray')  # linha de perfeição
plt.title('Treino vs Teste R²')
plt.xlabel('R² no Treino')
plt.ylabel('R² no Teste')
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib
# 1. Your final chosen hyperparameters
parametros_escolhidos = {
    'n_estimators': 200,
    'min_samples_split': 2,
    'min_samples_leaf': 3,
    'max_features': 'sqrt',
    'max_depth': 10
}

# 2. Pipeline with only the final model
modelo_final_30 = Pipeline(steps=[
    ('preprocessamento', preprocessor_30),  # ✅ Inclui transformações
    ('regressor', RandomForestRegressor(**parametros_escolhidos, random_state=42)) ###
])

# 3. Fit using preprocessed data
modelo_final_30.fit(X_train_30, y_train_30)

# Previsões no teste
y_pred_trans = modelo_final_30.predict(X_test_30)

# Reverter o target se foi transformado (como com PowerTransformer)
y_real = pt.inverse_transform(y_test_30).ravel()
y_pred = pt.inverse_transform(y_pred_trans.reshape(-1, 1)).ravel()

print("📊 Avaliação do modelo escolhido:")
print("MAE:", mean_absolute_error(y_real, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_real, y_pred)))
print("R²:", r2_score(y_real, y_pred))

joblib.dump(modelo_final_30, 'modelo_final_30_randomforest.pkl')
print("✅ Modelo salvo como 'modelo_final_30_randomforest.pkl")

import os

os.path.exists('modelo_final_30_randomforest.pkl')

"""## Prediction Scatter Analysis (Análise de Dispersão das Previsões)
Graph Interpretation
(Interpretação do Gráfico)

* Red dashed line represents the ideal scenario where predictions are equal to real values. (A linha vermelha tracejada representa o cenário ideal onde os valores previstos são iguais aos reais.)
* Blue dots are actual prediction points. (Os pontos azuis são as previsões reais do modelo.)

1. Most points follow a diagonal trend, indicating that the model captures the general pattern of the data.
(A maioria dos pontos segue uma tendência diagonal, indicando que o modelo capta o padrão geral dos dados.)

2. A noticeable vertical dispersion is observed, especially for higher real values (above 10), where predicted values tend to underestimate the true values.
(Observa-se uma dispersão vertical considerável, especialmente para valores reais mais altos (acima de 10), onde os valores previstos tendem a subestimar os reais.)

3. For low real values (between 0 and 5), the model performs better with tighter clusters along the ideal line.
(Para valores reais baixos (entre 0 e 5), o modelo apresenta melhor desempenho com pontos mais próximos da linha ideal.)

4. Even when real values approach 25, predictions rarely exceed 20.
(Mesmo quando os valores reais se aproximam de 25, as previsões raramente passam de 20.)
"""

plt.figure(figsize=(6,6))
plt.scatter(y_real, y_pred, alpha=0.3)
plt.plot([y_real.min(), y_real.max()], [y_real.min(), y_real.max()], 'r--')
plt.xlabel('Valor Real')
plt.ylabel('Valor Previsto')
plt.title('Dispersão: Valor Real vs Previsto')
plt.grid(True)
plt.tight_layout()
plt.show()

"""##Error Analysis (Análise dos Erros)
Boxplot Interpretation by Real Value Ranges
(Interpretação do Boxplot por Faixas de Valor Real)

* For lower real values (0–2 and 2–5), the absolute errors are small and tightly distributed.
(Para valores reais baixos (0–2 e 2–5), os erros absolutos são pequenos e bem concentrados.)
* As the real value increases, the median error and its variability also increase.
(À medida que o valor real aumenta, a mediana do erro e sua variabilidade também aumentam.)
* The range (interquartile range) and number of outliers grow significantly for higher brackets (e.g., 15–20 and 20–26).
(O intervalo interquartílico e o número de outliers crescem significativamente nas faixas mais altas (ex: 15–20 e 20–26).)
"""

# Error by true value bin
# (Erro por faixa do valor real)

df_erro = pd.DataFrame({
    'y_real': y_real,
    'y_pred': y_pred
})
df_erro['erro_absoluto'] = abs(df_erro['y_real'] - df_erro['y_pred'])
df_erro['faixa'] = pd.cut(df_erro['y_real'], bins=[0, 2, 5, 10, 15, 20, 26])

plt.figure(figsize=(10, 4))
sns.boxplot(data=df_erro, x='faixa', y='erro_absoluto')
plt.title('Erro absoluto por faixa de valor real')
plt.ylabel('Erro Absoluto')
plt.xlabel('Faixa de Valor Real')
plt.grid(True)
plt.show()

"""# Strategic Decision Based on Error Analysis

* The three final models (RandomForest) show similar behavior in both the scatter plots and the absolute error distributions.
(Os três modelos finalistas (RandomForest, LightGBM e GradientBoosting) apresentam comportamentos semelhantes nos gráficos de dispersão e nas distribuições de erro absoluto.)

* Most real user orders are concentrated between 1 and 6 orders per month — the region where the models are most accurate.
(A maior parte dos pedidos reais dos usuários está concentrada entre 1 e 6 pedidos/mês — exatamente onde os modelos são mais precisos.)

* Errors increase for higher values, but these cases are less frequent and naturally more variable.
(Os erros aumentam para valores mais altos, mas esses casos são menos frequentes na base e apresentam maior variabilidade natural.)

## Conclusion
* Accepting the current models is a smart choice for an initial modeling version, focusing on the majority of cases and ensuring good generalization where data is more abundant.
(Aceitar os modelos atuais é uma decisão prudente para uma primeira versão da modelagem, focando na maioria dos casos e garantindo boa generalização onde há mais dados.)

## Suggested Next Steps
* Analyze systematic errors for high-volume orders and identify patterns.
(Analisar erros sistemáticos para casos de alto volume e identificar padrões.)

* Create derived features that better capture the behavior of more active customers.
(Criar variáveis derivadas que capturem melhor o comportamento de clientes mais ativos.)

* Experiment with target transformations such as log1p or Yeo-Johnson, and compare performance, especially in upper value ranges.
(Experimentar transformações no target como log1p ou Yeo-Johnson e comparar o desempenho, principalmente nas faixas superiores.)

* Test weighted loss functions such as weighted MAE to reduce the impact of large errors in underperforming regions.
(Testar funções de perda com pesos, como MAE ponderado, para reduzir o impacto dos grandes erros onde o modelo apresenta pior desempenho.)

# 12. Prediction
"""

df_aug_missing_copy.head()

df_aug_missing_copy.info()

df_aug_sales_copy.head()

df_aug_sales_copy.info()

# Keep only the account_id from the second DataFrame using the first DataFrame's information
# Filter rows where account_id exists in the reference list
# (Filtrar as linhas onde o account_id existe na lista de referência)
filtered_df = df_aug_missing_copy[df_aug_missing_copy['account_id'].isin(df_aug_sales_copy['account_id'])]

filtered_df.head()

filtered_df.info()

diagnostico = diagnostico_vazios(filtered_df)
display(diagnostico)

# Generate random dates within August 2022
# (Gerar datas aleatórias dentro de agosto de 2022)
num_missing = filtered_df['order_date'].isna().sum()
random_dates = pd.to_datetime(
    np.random.choice(pd.date_range('2022-08-01', '2022-08-31'), size=num_missing)
)

# Create a mask to find NaT values
# (Criar uma máscara para encontrar os valores ausentes)
mask_na = filtered_df['order_date'].isna()

# Replace missing values with the random dates
# (Substituir os valores ausentes pelas datas aleatórias)
filtered_df.loc[mask_na, 'order_date'] = random_dates

# 1. Calculate the median of transaction amount from historical data
# (Calcular a mediana do valor de transação a partir dos dados históricos)
median_transaction = df_hist_end['total_transaction_amount'].median()

# 2. Fill missing values in the current DataFrame using the historical median
# (Preencher os valores ausentes no DataFrame atual usando a mediana histórica)
filtered_df['transaction_amount'] = filtered_df['transaction_amount'].fillna(median_transaction)

filtered_df.tail()

diagnostico = diagnostico_vazios(filtered_df)
display(diagnostico)

# Count the number of unique account_id in the filtered dataset
# (Contar o número de account_id distintos no conjunto filtrado)
num_accounts = filtered_df['account_id'].nunique()

print(f"Total unique account_id: {num_accounts}")
# (Total de account_id únicos: {num_accounts})

# 1. Group by client, date and transaction value, count how many times each appears
# (Agrupar por cliente, data e valor da transação, contando quantas vezes cada combinação aparece)
grouped = filtered_df.groupby(
    ['account_id', 'order_date', 'transaction_amount']
).size().reset_index(name='count')

# 2. Keep only combinations that occur more than once (i.e., are duplicated)
# (Manter apenas combinações que ocorrem mais de uma vez – ou seja, são duplicadas)
duplicated_orders = grouped[grouped['count'] > 1]

# 3. Sum total number of duplicated rows across all cases
# (Somar o número total de linhas duplicadas em todos os casos)
total_duplicated_rows = duplicated_orders['count'].sum()

# 4. Count how many duplicated groups exist (client + date + value)
# (Contar quantos grupos duplicados existem – cliente + data + valor)
num_duplicated_groups = len(duplicated_orders)

# 5. Count how many distinct clients had duplications
# (Contar quantos clientes distintos tiveram duplicações)
num_clients_with_duplicates = duplicated_orders['account_id'].nunique()

# 6. Print summary
# (Imprimir resumo)
print(f"Total duplicated rows: {total_duplicated_rows}")
# (Total de linhas duplicadas: {total_duplicated_rows})

print(f"Number of duplicated groups (client + date + value): {num_duplicated_groups}")
# (Número de grupos duplicados (cliente + data + valor): {num_duplicated_groups})

print(f"Number of distinct clients with duplications: {num_clients_with_duplicates}")
# (Número de clientes distintos com duplicações: {num_clients_with_duplicates})

# 1. Get the minimum and maximum duplicated transaction values
# (Obter o menor e o maior valor de transação duplicado)
min_duplicated_value = duplicated_orders['transaction_amount'].min()
max_duplicated_value = duplicated_orders['transaction_amount'].max()

# 2. Count how many times each of them appears in duplicated records
# (Contar quantas vezes cada um aparece nos registros duplicados)
min_value_count = duplicated_orders[duplicated_orders['transaction_amount'] == min_duplicated_value]['count'].sum()

max_value_count = duplicated_orders[duplicated_orders['transaction_amount'] == max_duplicated_value]['count'].sum()

# 3. Print results
# (Imprimir os resultados)
print(f"Minimum duplicated value: {min_duplicated_value} – Occurrences: {min_value_count}")
# (Menor valor duplicado: {min_duplicated_value} – Ocorrências: {min_value_count})

print(f"Maximum duplicated value: {max_duplicated_value} – Occurrences: {max_value_count}")
# (Maior valor duplicado: {max_duplicated_value} – Ocorrências: {max_value_count})

filtered_df.head()

# Remove duplicated rows keeping the first occurrence
# (Remover linhas duplicadas mantendo a primeira ocorrência)

df_dedup = filtered_df.drop_duplicates(
    subset=['account_id', 'order_date', 'transaction_amount'],
    keep='first'
).reset_index(drop=True)

print(f"Original shape: {filtered_df.shape}")  # (Tamanho original)
print(f"After dedup   : {df_dedup.shape}")     # (Após deduplicar)

# Count the number of unique account_id in the filtered dataset
# (Contar o número de account_id distintos no conjunto filtrado)
num_accounts = df_dedup['account_id'].nunique()

print(f"Total unique account_id: {num_accounts}")
# (Total de account_id únicos: {num_accounts})

df_aug_missing_copy = df_dedup.copy()

# 1. Convert the date column to datetime format (Converte a coluna de data para o formato datetime)
df_aug_missing_copy["order_date"] = pd.to_datetime(df_aug_missing_copy["order_date"])

# 2. Create date-related features (Cria colunas relacionadas à data)
df_aug_missing_copy["year"] = df_aug_missing_copy["order_date"].dt.year  # Year (Ano)
df_aug_missing_copy["month"] = df_aug_missing_copy["order_date"].dt.month  # Month (Mês)
df_aug_missing_copy["day"] = df_aug_missing_copy["order_date"].dt.day  # Day of month (Dia do mês)
df_aug_missing_copy["days_of_the_week"] = df_aug_missing_copy["order_date"].dt.weekday  # Day of week (Dia da semana: 0=Mon, 6=Sun)
df_aug_missing_copy["year_month"] = df_aug_missing_copy["order_date"].dt.to_period("M")  # Year-Month (Ano-Mês)

# 3. Define Brazilian national holidays (Define feriados nacionais do Brasil)
br_holidays = [
    "2021-01-01",  # New Year's Day (Ano Novo)
    "2021-04-21",  # Tiradentes
    "2021-05-01",  # Labor Day (Dia do Trabalho)
    "2021-09-07",  # Independence Day (Independência)
    "2021-10-12",  # Our Lady of Aparecida (Padroeira do Brasil)
    "2021-11-02",  # All Souls' Day (Finados)
    "2021-11-15",  # Proclamation of the Republic (Proclamação da República)
    "2021-12-25",  # Christmas (Natal)
    # Add more years if needed (Adicione mais anos se necessário)
]

# 4. Convert holidays to datetime (Converte os feriados para datetime)
feriados_nacionais = pd.to_datetime(br_holidays)

# 5. Add weekend indicator (Adiciona indicador de fim de semana)
df_aug_missing_copy["is_weekend"] = df_aug_missing_copy["order_date"].dt.weekday >= 5  # Saturday or Sunday (Sábado ou Domingo)

# 6. Add holiday indicator (Adiciona indicador de feriado nacional)
df_aug_missing_copy["is_holiday"] = df_aug_missing_copy["order_date"].isin(feriados_nacionais)

# 7. Add business day indicator (Adiciona indicador de dia útil)
df_aug_missing_copy["is_business_day"] = ~df_aug_missing_copy["is_weekend"] & ~df_aug_missing_copy["is_holiday"]  # Not weekend or holiday (Não é fim de semana nem feriado)

#df_aug_missing_copy['year_month'] = df_aug_missing_copy['year_month'].dt.to_timestamp()

df_aug_missing_copy.head()

df_aug_missing_copy.info()

# 1. Aggregate transaction amount per client per day
# (Agregar o valor das transações por cliente e por dia)
df_agg = df_aug_missing_copy.groupby([
    "account_id", "order_date", "year", "month", "day", "year_month",
    "days_of_the_week", "is_weekend", "is_holiday", "is_business_day"
]).agg({"transaction_amount": "sum"}).rename(
    columns={"transaction_amount": "total_transaction_amount"}  # (Renomear para total_transaction_amount)
).reset_index()

# 3. Create 'cutoff' column based on day of the month
# (Criar a coluna 'cutoff' com base no dia do mês)
def define_cutoff(day):
    if day <= 10:
        return 10
    elif day <= 20:
        return 20
    else:
        return 30

df_agg["cutoff"] = df_agg["day"].apply(define_cutoff)

# 3. Create 'has_order': 1 if total_transaction_amount > 0, else 0
# (Criar 'has_order': 1 se o valor total do dia for positivo, senão 0)
df_agg["has_order"] = df_agg["total_transaction_amount"].apply(
    lambda x: 1 if x > 0 else 0
)

# 4. Create column 'total_orders_in_month' based on has_order
# (Criar a coluna 'total_orders_in_month' com base em has_order)
orders_per_month = df_agg.groupby(["account_id", "year_month"])["has_order"] \
    .sum().reset_index().rename(columns={"has_order": "total_orders_in_month"})

# 5. Merge monthly totals back into df_agg
# (Juntar o total mensal de pedidos no DataFrame agregado)
df_agg = df_agg.merge(orders_per_month, on=["account_id", "year_month"], how="left")

# 6. Create 'qtd_orders': number of positive-value transactions per client per day
# (Criar 'qtd_orders': número de transações com valor positivo por cliente por dia)
df_pos = df_aug_missing_copy[df_aug_missing_copy["transaction_amount"] > 0]
qtd_orders = df_pos.groupby(["account_id", "order_date"]).size().reset_index(name="qtd_orders")

# 7. Merge qtd_orders into df_agg
# (Juntar qtd_orders no df_agg)
df_agg = df_agg.merge(qtd_orders, on=["account_id", "order_date"], how="left")
df_agg["qtd_orders"] = df_agg["qtd_orders"].fillna(0).astype(int)

# 8. Force qtd_orders = 0 where has_order == 0
# (Forçar qtd_orders = 0 onde has_order == 0)
df_agg.loc[df_agg["has_order"] == 0, "qtd_orders"] = 0

df_agg.head()

# Count the number of unique account_id in the filtered dataset
# (Contar o número de account_id distintos no conjunto filtrado)
num_accounts = df_agg['account_id'].nunique()

print(f"Total unique account_id: {num_accounts}")
# (Total de account_id únicos: {num_accounts})

diagnostico = diagnostico_vazios(df_agg)
display(diagnostico)

# Check the distribution of the target column has_order
#(Verificar a distribuição da variável alvo has_order)
df_agg['has_order'].value_counts(normalize=True) * 100

# Validates that we're not counting orders on days where the client shouldn't have any.
#(Valida que não estamos contando pedidos em dias onde o cliente não deveria ter feito nenhum.)
df_agg[df_agg['has_order'] == 0]['qtd_orders'].unique()

df_agg[df_agg['has_order'] == 0].head()

df_agg[(df_agg['has_order'] == 0) & (df_agg['qtd_orders'] > 0)]

# Reorder the columns in the desired order
# (Reorganizar as colunas na ordem desejada)

ordered_columns = [
    'account_id',
    'year_month',
    'cutoff',
    'order_date',
    'year',
    'month',
    'day',
    'days_of_the_week',
    'is_weekend',
    'is_holiday',
    'is_business_day',
    'total_transaction_amount',
    'has_order',
    'qtd_orders',
    'total_orders_in_month'
]

# Apply the new order to the DataFrame
# (Aplicar a nova ordem ao DataFrame)
df_agg = df_agg[ordered_columns]

# Optional: preview the result
# (Opcional: visualizar o resultado)
df_agg.head()

df_agg[(df_agg['has_order'] == 0) & (df_agg['total_orders_in_month'] == 0)]

df_agg[(df_agg['has_order'] == 0) & (df_agg['total_orders_in_month'] > 0) & (df_agg['qtd_orders'] == 0)]

df_agg[df_agg['account_id'] == 'BR_00069147213']

# Step 1: Remove inconsistent rows
# (Passo 1: Remover linhas inconsistentes)
cond_invalido = (
    (df_agg['has_order'] == 0) &
    (df_agg['qtd_orders'] == 0) &
    (df_agg['total_orders_in_month'] > 0)
)
df_filtrado = df_agg[~cond_invalido].copy()

# Step 2: Keep only the first occurrence of each account_id per cutoff, based on order_date
# (Passo 2: Manter apenas a primeira ocorrência de cada account_id por corte, com base na menor data)
df_filtrado = (
    df_filtrado.sort_values(by=['account_id', 'cutoff', 'order_date'])  # (Ordena para priorizar a menor data)
    .drop_duplicates(subset=['account_id', 'cutoff'], keep='first')     # (Remove duplicados, mantendo o primeiro)
    .reset_index(drop=True)
)
# Step 3: Apply cutoff priority rules for duplicated account_ids across different cutoffs
# (Passo 3: Aplicar regras de prioridade entre cortes para account_ids duplicados)
# Count how many times each account_id appears (Contar quantas vezes cada account_id aparece)
contagem = df_filtrado['account_id'].value_counts()
ids_duplicados = contagem[contagem > 1].index

# Separate duplicated and non-duplicated account_ids
# (Separar account_ids duplicados e não duplicados)
df_nao_duplicado = df_filtrado[~df_filtrado['account_id'].isin(ids_duplicados)].copy()
df_duplicado = df_filtrado[df_filtrado['account_id'].isin(ids_duplicados)].copy()

# Define a priority order for cutoff (Definir a ordem de prioridade dos cortes)
cutoff_prioridade = {10: 0, 20: 1, 30: 2}

# Map priority (Mapear a prioridade)
df_duplicado['prioridade'] = df_duplicado['cutoff'].map(cutoff_prioridade)

# Sort and keep the highest priority row per account_id
# (Ordenar e manter apenas a linha com maior prioridade por account_id)
df_duplicado = (
    df_duplicado.sort_values(by=['account_id', 'prioridade'])  # (Menor valor de prioridade = mais importante)
    .drop_duplicates(subset=['account_id'], keep='first')      # (Mantém apenas a linha com maior prioridade)
    .drop(columns='prioridade')                                # (Remove a coluna auxiliar)
)

# Concatenate back with non-duplicated rows
# (Concatenar novamente com os registros não duplicados)
df_final = pd.concat([df_nao_duplicado, df_duplicado], ignore_index=True)

df_final[df_final['account_id'] == 'BR_00069147213']

df_final[(df_final['has_order'] == 0) & (df_final['total_orders_in_month'] > 0) & (df_final['qtd_orders'] == 0)]

df_final[(df_final['has_order'] == 0) & (df_final['total_orders_in_month'] == 0)]

df_cuts_per_user_month = df_final.groupby(['account_id', 'year_month'])['cutoff'].nunique()
print(df_cuts_per_user_month.value_counts().sort_index())

df_final.info()

# 1. Count the total number of rows for each cutoff
# (1. Contar o número total de registros para cada corte)
cutoff_counts = df_final['cutoff'].value_counts().sort_index()

# 2. Count how many rows have has_order == 1 per cutoff
# (2. Contar quantos registros têm has_order == 1 por corte)
cutoff_has_order_1 = df_final[df_final['has_order'] == 1]['cutoff'].value_counts().sort_index()

# 3. Count how many rows have has_order == 0 per cutoff
# (3. Contar quantos registros têm has_order == 0 por corte)
cutoff_has_order_0 = df_final[df_final['has_order'] == 0]['cutoff'].value_counts().sort_index()

# 4. Combine the counts into a single DataFrame
# (4. Combinar os resultados em um único DataFrame)
cutoff_summary = pd.DataFrame({
    'total_records': cutoff_counts,         # (total de registros)
    'has_order_1': cutoff_has_order_1,      # (quantos com has_order == 1)
    'has_order_0': cutoff_has_order_0       # (quantos com has_order == 0)
}).fillna(0).astype(int)  # Fill missing values with 0 and convert to int (Preencher valores ausentes com 0 e converter para inteiro)

# 5. Display the summary
# (5. Exibir o resumo)
print("Summary of records per cutoff (Resumo dos registros por corte):")
print(cutoff_summary)

# Rename the column to 'num_days_with_order'
# (Renomear a coluna para 'num_days_with_order')
df_final = df_final.rename(columns={'total_orders_in_month': 'num_days_with_order'})

df_final.head()

# Separate data by cutoff.
# (Separar os dados por corte)
df_corte_10 = df_final[df_final['cutoff'] == 10].copy()
df_corte_20 = df_final[df_final['cutoff'] == 20].copy()
df_corte_30 = df_final[df_final['cutoff'] == 30].copy()

df_corte_10.head()

# Create column: cutoff_day if order, else 0
# (Criar coluna 'cutoff_day_if_order')
df_corte_10['cutoff_day_if_order'] = np.where(
    df_corte_10['has_order'] == 1,
    df_corte_10['cutoff'] -  df_corte_10['day'],
    0
)

df_corte_20['cutoff_day_if_order'] = np.where(
    df_corte_20['has_order'] == 1,
    df_corte_20['cutoff'] -  df_corte_20['day'],
    0
)

df_corte_30['cutoff_day_if_order'] = np.where(
    df_corte_30['has_order'] == 1,
    df_corte_30['cutoff'] -  df_corte_30['day'],
    0
)

# Ticket médio por pedido
# (Average ticket per order — avoid division by zero)
df_corte_10['avg_ticket_per_day'] = np.where(
    df_corte_10['qtd_orders'] == 0, 0,
    df_corte_10['total_transaction_amount'] / df_corte_10['qtd_orders'],
).round(2)

df_corte_20['avg_ticket_per_day'] = np.where(
    df_corte_20['qtd_orders'] == 0, 0,
    df_corte_20['total_transaction_amount'] / df_corte_20['qtd_orders'],
).round(2)

df_corte_30['avg_ticket_per_day'] = np.where(
    df_corte_30['qtd_orders'] == 0, 0,
    df_corte_30['total_transaction_amount'] / df_corte_30['qtd_orders'],

).round(2)

# Calculates the average amount spent per cut
# (Calcula a medía de valores gastos por corte)
df_corte_10['avg_ticket_per_cutoff'] = (df_corte_10['total_transaction_amount'] / 10).round(2)

df_corte_20['avg_ticket_per_cutoff'] = (df_corte_20['total_transaction_amount'] / 10).round(2)

df_corte_30['avg_ticket_per_cutoff'] = (df_corte_30['total_transaction_amount'] / 10).round(2)

# Calculate the monthly conversion rate as the ratio between days with order and total days until cutoff
# (Calcula a taxa de conversão mensal como a razão entre os dias com pedido e o total de dias até o corte)
df_corte_10['conversion_rate_10_days'] = (df_corte_10['num_days_with_order'] / 10).round(2)

df_corte_20['conversion_rate_10_days'] = (df_corte_20['num_days_with_order'] / 10).round(2)

df_corte_30['conversion_rate_10_days'] = (df_corte_30['num_days_with_order'] / 10).round(2)

# Calculate recency_score based on whether a purchase was made
# (Calcula a recency_score com base se houve pedido ou não)
df_corte_10['recency_score'] = np.where(
    df_corte_10['has_order'] == 1,
    1 - (df_corte_10['cutoff_day_if_order'] / 10),
    0
)

df_corte_20['recency_score'] = np.where(
    df_corte_20['has_order'] == 1,
    1 - (df_corte_20['cutoff_day_if_order'] / 10),
    0
)

df_corte_30['recency_score'] = np.where(
    df_corte_30['has_order'] == 1,
    1 - (df_corte_30['cutoff_day_if_order'] / 10),
    0
)

# Estimated probability of future orders based on current frequency
# (Probabilidade estimada de novos pedidos com base na frequência atual)
# Probability that the customer will make new orders in the remaining days, assuming the current rate continues
# (Probabilidade de o cliente realizar novos pedidos nos dias restantes, assumindo que a taxa atual se mantém)
df_corte_10['remaining_days'] = 30 - df_corte_10['cutoff']  # Number of days left in the month (Número de dias restantes no mês)
df_corte_10['p_future_order'] = (df_corte_10['num_days_with_order'] * df_corte_10['remaining_days']) / (df_corte_10['cutoff'] ** 2)

df_corte_20['remaining_days'] = 30 - df_corte_20['cutoff']  # Number of days left in the month (Número de dias restantes no mês)
df_corte_20['p_future_order'] = (df_corte_20['num_days_with_order'] * df_corte_20['remaining_days']) / (df_corte_20['cutoff'] ** 2)

df_corte_30['remaining_days'] = 30 - df_corte_30['cutoff']  # Number of days left in the month (Número de dias restantes no mês)
df_corte_30['p_future_order'] = (df_corte_30['num_days_with_order'] * df_corte_30['remaining_days']) / (df_corte_30['cutoff'] ** 2)

# Average number of orders per day with order (intensity of purchase)
# (Média de pedidos por dia com pedido - intensidade da compra)
# How many orders the customer makes on average in the days when they place at least one order
# (Quantos pedidos o cliente faz em média nos dias em que realiza pelo menos um pedido)
df_corte_10['avg_orders_per_order_day'] = np.where(
    df_corte_10['num_days_with_order'] == 0, 0,
    df_corte_10['qtd_orders'] / df_corte_10['num_days_with_order']
)

df_corte_20['avg_orders_per_order_day'] = np.where(
    df_corte_20['num_days_with_order'] == 0, 0,
    df_corte_20['qtd_orders'] / df_corte_20['num_days_with_order']
)

df_corte_30['avg_orders_per_order_day'] = np.where(
    df_corte_30['num_days_with_order'] == 0, 0,
    df_corte_30['qtd_orders'] / df_corte_30['num_days_with_order']
)

# Probability of having at least 5 orders in the month (binomial model)
# (Probabilidade de ter pelo menos 5 pedidos no mês - modelo binomial)
# Estimated daily success probability for orders based on current data
# (Probabilidade estimada de sucesso diário com base no comportamento atual)
# Probability of reaching 5 or more order days in the full month, assuming binomial distribution
# (Probabilidade de atingir 5 ou mais dias com pedido no mês inteiro, assumindo distribuição binomial)
df_corte_10['p_binom'] = df_corte_10['num_days_with_order'] / df_corte_10['cutoff']
df_corte_10['p_at_least_5_orders_binom'] = binom.sf(k=4, n=30, p=df_corte_10['p_binom'])

df_corte_20['p_binom'] = df_corte_20['num_days_with_order'] / df_corte_20['cutoff']
df_corte_20['p_at_least_5_orders_binom'] = binom.sf(k=4, n=30, p=df_corte_20['p_binom'])

df_corte_30['p_binom'] = df_corte_30['num_days_with_order'] / df_corte_30['cutoff']
df_corte_30['p_at_least_5_orders_binom'] = binom.sf(k=4, n=30, p=df_corte_30['p_binom'])

#interaction of attributes
df_corte_10['num_days_with_order_X_cutoff_day_if_order'] = df_corte_10['num_days_with_order'] * df_corte_10['cutoff_day_if_order']

df_corte_20['num_days_with_order_X_cutoff_day_if_order'] = df_corte_20['num_days_with_order'] * df_corte_20['cutoff_day_if_order']

df_corte_30['num_days_with_order_X_cutoff_day_if_order'] = df_corte_30['num_days_with_order'] * df_corte_30['cutoff_day_if_order']

###############################################################################################################################################

df_corte_10['total_transaction_amount_X_cutoff_day_if_order'] = np.where(
    df_corte_10['cutoff_day_if_order'] == 0, 0,
    df_corte_10['total_transaction_amount'] * df_corte_10['cutoff_day_if_order']
).round(2)

df_corte_20['total_transaction_amount_X_cutoff_day_if_order'] = np.where(
    df_corte_20['cutoff_day_if_order'] == 0, 0,
    df_corte_20['total_transaction_amount'] * df_corte_20['cutoff_day_if_order']
).round(2)

df_corte_30['total_transaction_amount_X_cutoff_day_if_order'] = np.where(
    df_corte_30['cutoff_day_if_order'] == 0, 0,
    df_corte_30['total_transaction_amount'] * df_corte_30['cutoff_day_if_order']
).round(2)

###############################################################################################################################################

df_corte_10['day_X_has_order'] = np.where(
    df_corte_10['num_days_with_order'] == 0, 0,
    df_corte_10['num_days_with_order'] / df_corte_10['day']
)

df_corte_20['day_X_has_order'] = np.where(
    df_corte_20['num_days_with_order'] == 0, 0,
    df_corte_20['num_days_with_order'] / df_corte_20['day']
)

df_corte_30['day_X_has_order'] = np.where(
    df_corte_30['num_days_with_order'] == 0, 0,
    df_corte_30['num_days_with_order'] / df_corte_30['day']
)

df_predict_10 = df_corte_10.copy()
df_predict_20 = df_corte_20.copy()
df_predict_30 = df_corte_30.copy()

# Create an empty column named 'total_orders_in_month'
# (Criar uma coluna vazia chamada 'total_orders_in_month')
df_predict_10['total_orders_in_month'] = np.nan
df_predict_20['total_orders_in_month'] = np.nan
df_predict_30['total_orders_in_month'] = np.nan

df_predict_10.head()

target_10 = 'total_orders_in_month'
features_10 = [
            'year', 'month', 'day', 'days_of_the_week',
            'is_weekend', 'is_holiday', 'is_business_day',
            'total_transaction_amount', 'has_order', 'qtd_orders',
            'num_days_with_order', 'cutoff_day_if_order', 'avg_ticket_per_day',
            'avg_ticket_per_cutoff',  'conversion_rate_10_days', 'recency_score',
            'p_future_order', 'avg_orders_per_order_day',
            'p_binom', 'p_at_least_5_orders_binom', 'num_days_with_order_X_cutoff_day_if_order',
            'total_transaction_amount_X_cutoff_day_if_order', 'day_X_has_order'
]

import joblib
modelo_carregado_10 = joblib.load('modelo_final_10_randomforest.pkl')

# Ensure index is sequential before prediction
# (Garantir que o índice seja sequencial antes da previsão)
df_predict_10 = df_predict_10.reset_index(drop=True)

# Step 1: Select the same features used in training
# (Etapa 1: Selecionar as mesmas variáveis usadas no treinamento)
X_10 = df_predict_10[features_10]

# Step 2: Predict with the loaded model
# (Etapa 2: Fazer a previsão com o modelo carregado)
y_pred_10_transformed = modelo_carregado_10.predict(X_10)

# Step 3: Reverse the PowerTransformer transformation on the target
# (Etapa 3: Reverter a transformação do PowerTransformer no target)
y_pred_10 = pt.inverse_transform(y_pred_10_transformed.reshape(-1, 1)).flatten()

# Step 4: Create a result DataFrame with account_id, num_days_with_order, and predictions
# (Etapa 4: Criar um DataFrame de resultado com account_id, num_days_with_order e previsões)
df_resultado_10 = pd.DataFrame({
    'account_id': df_predict_10['account_id'].values,
    'num_days_with_order': df_predict_10['num_days_with_order'].values,
    'total_orders_in_month': y_pred_10
})

df_resultado_10.head()

df_resultado_10_final = df_resultado_10.copy()

# 1) Round total_orders_in_month to 2 decimals
# (Arredondar para 2 casas decimais)
df_resultado_10_final['total_orders_in_month'] = df_resultado_10_final['total_orders_in_month'].round(0).astype(int)

# 2) Create a new column with the difference
# (Criar uma nova coluna com a diferença)
df_resultado_10_final['orders_minus_days'] = (
    df_resultado_10_final['total_orders_in_month'] - df_resultado_10_final['num_days_with_order']
).astype(int)

df_resultado_10_final.head(20)

target_20 = 'total_orders_in_month'
features_20 = [
            'year_month','year', 'month', 'day', 'days_of_the_week',
            'is_weekend', 'is_holiday', 'is_business_day',
            'total_transaction_amount', 'has_order', 'qtd_orders',
            'num_days_with_order', 'cutoff_day_if_order', 'avg_ticket_per_day',
            'avg_ticket_per_cutoff', 'conversion_rate_10_days', 'recency_score',
            'p_future_order', 'avg_orders_per_order_day',
            'p_binom', 'p_at_least_5_orders_binom', 'num_days_with_order_X_cutoff_day_if_order',
            'total_transaction_amount_X_cutoff_day_if_order', 'day_X_has_order'
]

modelo_carregado_20 = joblib.load('modelo_final_20_randomforest.pkl')

# Ensure index is sequential before prediction
# (Garantir que o índice seja sequencial antes da previsão)
df_predict_20 = df_predict_20.reset_index(drop=True)

# Step 1: Select the same features used in training
# (Etapa 1: Selecionar as mesmas variáveis usadas no treinamento)
X_20 = df_predict_20[features_20]

# Step 2: Predict with the loaded model
# (Etapa 2: Fazer a previsão com o modelo carregado)
y_pred_20_transformed = modelo_carregado_20.predict(X_20)

# Step 3: Reverse the PowerTransformer transformation on the target
# (Etapa 3: Reverter a transformação do PowerTransformer no target)
y_pred_20 = pt.inverse_transform(y_pred_20_transformed.reshape(-1, 1)).flatten()

# Step 4: Create a result DataFrame with account_id, num_days_with_order, and predictions
# (Etapa 4: Criar um DataFrame de resultado com account_id, num_days_with_order e previsões)
df_resultado_20 = pd.DataFrame({
    'account_id': df_predict_20['account_id'].values,
    'num_days_with_order': df_predict_20['num_days_with_order'].values,
    'total_orders_in_month': y_pred_20
})

df_resultado_20.head()

df_resultado_20_final = df_resultado_20.copy()

# 1) Round total_orders_in_month to 2 decimals
# (Arredondar para 2 casas decimais)
df_resultado_20_final['total_orders_in_month'] = df_resultado_20_final['total_orders_in_month'].round(0).astype(int)

# 2) Create a new column with the difference
# (Criar uma nova coluna com a diferença)
df_resultado_20_final['orders_minus_days'] = (
    df_resultado_20_final['total_orders_in_month'] - df_resultado_20_final['num_days_with_order']
).astype(int)

df_resultado_20_final.head(20)

target_30 = 'total_orders_in_month'
features_30 = [
            'year', 'month', 'day', 'days_of_the_week',
            'is_weekend', 'is_holiday', 'is_business_day',
            'total_transaction_amount', 'has_order', 'qtd_orders',
            'num_days_with_order', 'cutoff_day_if_order', 'avg_ticket_per_day',
            'avg_ticket_per_cutoff', 'conversion_rate_10_days', 'recency_score',
            'avg_orders_per_order_day', 'p_binom', 'p_at_least_5_orders_binom',
            'num_days_with_order_X_cutoff_day_if_order', 'total_transaction_amount_X_cutoff_day_if_order',
            'day_X_has_order'
]

modelo_carregado_30 = joblib.load('modelo_final_30_randomforest.pkl')

# Ensure index is sequential before prediction
# (Garantir que o índice seja sequencial antes da previsão)
df_predict_30 = df_predict_30.reset_index(drop=True)

# Step 1: Select the same features used in training
# (Etapa 1: Selecionar as mesmas variáveis usadas no treinamento)
X_30 = df_predict_30[features_30]

# Step 2: Predict with the loaded model
# (Etapa 2: Fazer a previsão com o modelo carregado)
y_pred_30_transformed = modelo_carregado_30.predict(X_30)

# Step 3: Reverse the PowerTransformer transformation on the target
# (Etapa 3: Reverter a transformação do PowerTransformer no target)
y_pred_30 = pt.inverse_transform(y_pred_30_transformed.reshape(-1, 1)).flatten()

# Step 4: Create a result DataFrame with account_id, num_days_with_order, and predictions
# (Etapa 4: Criar um DataFrame de resultado com account_id, num_days_with_order e previsões)
df_resultado_30 = pd.DataFrame({
    'account_id': df_predict_30['account_id'].values,
    'num_days_with_order': df_predict_30['num_days_with_order'].values,
    'total_orders_in_month': y_pred_30
})

df_resultado_30.head()

df_resultado_30_final = df_resultado_30.copy()

# 1) Round total_orders_in_month to 2 decimals
# (Arredondar para 2 casas decimais)
df_resultado_30_final['total_orders_in_month'] = df_resultado_30_final['total_orders_in_month'].round(0).astype(int)

# 2) Create a new column with the difference
# (Criar uma nova coluna com a diferença)
df_resultado_30_final['orders_minus_days'] = (
    df_resultado_30_final['total_orders_in_month'] - df_resultado_30_final['num_days_with_order']
).astype(int)

df_resultado_30_final.head(20)

df10 = df_resultado_10_final[['account_id', 'orders_minus_days']].copy()
df20 = df_resultado_20_final[['account_id', 'orders_minus_days']].copy()
df30 = df_resultado_30_final[['account_id', 'orders_minus_days']].copy()

# Rename column orders_minus_days to predict
# (Renomear coluna orders_minus_days para predict)
df10 = df10.rename(columns={'orders_minus_days': 'predict'})
df20 = df20.rename(columns={'orders_minus_days': 'predict'})
df30 = df30.rename(columns={'orders_minus_days': 'predict'})

order_days_prediction = pd.concat([df10, df20, df30], ignore_index=True)


print(order_days_prediction.shape)
order_days_prediction.head()

pip install reportlab

from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet

# Path to save the PDF file
# (Caminho para salvar o arquivo PDF)
output_path = "order_days_prediction.pdf"

# Create the PDF document
# (Criar o documento PDF)
styles = getSampleStyleSheet()
doc = SimpleDocTemplate(output_path, pagesize=A4)

# Convert DataFrame to a list of lists (header + rows)
# (Converter o DataFrame para uma lista de listas — cabeçalho + linhas)
data = [order_days_prediction.columns.tolist()] + order_days_prediction.values.tolist()

# Create the table with the data
# (Criar a tabela com os dados)
table = Table(data)

# Define the style for the table
# (Definir o estilo para a tabela)
table.setStyle(TableStyle([
    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),         # Header background color (Cor de fundo do cabeçalho)
    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),    # Header text color (Cor do texto do cabeçalho)
    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),                # Center align all cells (Centralizar todas as células)
    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),      # Bold font for header (Fonte em negrito para o cabeçalho)
    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),               # Padding below header text (Espaçamento abaixo do texto do cabeçalho)
    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),       # Background color for rows (Cor de fundo para as linhas)
    ('GRID', (0, 0), (-1, -1), 1, colors.black),          # Add grid lines (Adicionar linhas de grade)
]))

# Add title and table to the PDF
# (Adicionar título e tabela ao PDF)
elements = [Paragraph("Order Days Prediction", styles['Title']), table]

# Build the PDF file
# (Construir o arquivo PDF)
doc.build(elements)

print(f"PDF saved at: {output_path}")  # (PDF salvo em:)

import os, sys
from IPython.display import display

pdf_path = "order_days_prediction.pdf"

# Check file
# (Checar arquivo)
assert os.path.exists(pdf_path) and os.path.getsize(pdf_path) > 0, "PDF missing or empty (PDF ausente ou vazio)"

# Colab direct download
# (Download direto no Colab)
from google.colab import files  # (apenas no Colab)
files.download(pdf_path)